{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQVHjJD4j4ig",
        "outputId": "91d8f954-0966-474a-a009-428a4ee0790d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ]
        }
      ],
      "source": [
        "# run this cell if you are in colab with a single notebook opened, otherwise ignore this cell\n",
        "\n",
        "#!git clone https://github.com/CowboyPhilip/HPML-Energy-Efficient-LLM\n",
        "#%cd HPML-Energy-Efficient-LLM\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5G8E_rswj5Pf",
        "outputId": "af2df541-d972-4511-a253-d67c3f4acce2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (80.3.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Collecting bitsandbytes\n",
            "  Using cached bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting zeus-ml\n",
            "  Using cached zeus_ml-0.11.0.post1-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting datasets\n",
            "  Using cached datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting evaluate\n",
            "  Using cached evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: geocoder in /usr/local/lib/python3.11/dist-packages (1.38.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from zeus-ml) (2.2.2)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.11/dist-packages (from zeus-ml) (12.570.86)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from zeus-ml) (2.11.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from zeus-ml) (13.9.4)\n",
            "Collecting tyro (from zeus-ml)\n",
            "  Using cached tyro-0.9.19-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from zeus-ml) (0.28.1)\n",
            "Collecting amdsmi (from zeus-ml)\n",
            "  Using cached amdsmi-6.4.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from zeus-ml) (2.9.0.post0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
            "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from geocoder) (8.1.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from geocoder) (1.0.0)\n",
            "Requirement already satisfied: ratelim in /usr/local/lib/python3.11/dist-packages (from geocoder) (0.1.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from geocoder) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.27.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (80.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->zeus-ml) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->zeus-ml) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->zeus-ml) (0.4.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->zeus-ml) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->zeus-ml) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->zeus-ml) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->zeus-ml) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->zeus-ml) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->zeus-ml) (2025.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ratelim->geocoder) (4.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->zeus-ml) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->zeus-ml) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->zeus-ml) (0.1.2)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->zeus-ml) (0.16)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro->zeus-ml) (1.7.2)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->zeus-ml) (4.4.2)\n",
            "Using cached bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "Using cached zeus_ml-0.11.0.post1-py3-none-any.whl (227 kB)\n",
            "Using cached datasets-3.5.1-py3-none-any.whl (491 kB)\n",
            "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "Using cached evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "Using cached amdsmi-6.4.0-py3-none-any.whl (56 kB)\n",
            "Using cached tyro-0.9.19-py3-none-any.whl (124 kB)\n",
            "Installing collected packages: nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, amdsmi, nvidia-cusolver-cu12, nvidia-cudnn-cu12, multiprocess, tyro, zeus-ml, datasets, bitsandbytes, evaluate\n",
            "\u001b[2K  Attempting uninstall: nvidia-cusparse-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "\u001b[2K  Attempting uninstall: nvidia-curand-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "\u001b[2K  Attempting uninstall: nvidia-cufft-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "\u001b[2K    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "\u001b[2K  Attempting uninstall: nvidia-cublas-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "\u001b[2K    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "\u001b[2K  Attempting uninstall: fsspec\n",
            "\u001b[2K    Found existing installation: fsspec 2025.3.2\n",
            "\u001b[2K    Uninstalling fsspec-2025.3.2:\n",
            "\u001b[2K      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[2K  Attempting uninstall: nvidia-cusolver-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/18\u001b[0m [evaluate]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed amdsmi-6.4.0 bitsandbytes-0.45.5 datasets-3.5.1 dill-0.3.8 evaluate-0.4.3 fsspec-2025.3.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 tyro-0.9.19 zeus-ml-0.11.0.post1\n"
          ]
        }
      ],
      "source": [
        "# 1. Install dependencies\n",
        "!pip install --upgrade pip setuptools\n",
        "!pip install \\\n",
        "    transformers \\\n",
        "    bitsandbytes \\\n",
        "    zeus-ml \\\n",
        "    torch \\\n",
        "    datasets \\\n",
        "    evaluate \\\n",
        "    scikit-learn \\\n",
        "    geocoder \\\n",
        "    requests \\\n",
        "    numpy \\\n",
        "    wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwXrXRkoUWck",
        "outputId": "ab910035-ddf5-4da3-c7c6-d724ae0b2f24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting flash-attn==2.0.5\n",
            "  Using cached flash_attn-2.0.5.tar.gz (2.3 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting triton==2.0.0\n",
            "  Using cached triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.8.5.post1-cp38-abi3-manylinux1_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn==2.0.5) (2.6.0+cu124)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn==2.0.5) (0.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from flash-attn==2.0.5) (24.2)\n",
            "Collecting ninja (from flash-attn==2.0.5)\n",
            "  Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0) (3.31.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0) (3.18.0)\n",
            "Collecting lit (from triton==2.0.0)\n",
            "  Using cached lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from vllm) (5.5.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from vllm) (5.9.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from vllm) (0.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from vllm) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from vllm) (4.67.1)\n",
            "Collecting blake3 (from vllm)\n",
            "  Using cached blake3-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from vllm) (9.0.0)\n",
            "Requirement already satisfied: transformers>=4.51.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.51.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[hf_xet]>=0.30.0->vllm) (0.30.2)\n",
            "Requirement already satisfied: tokenizers>=0.21.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from vllm) (5.29.4)\n",
            "Collecting fastapi>=0.115.0 (from fastapi[standard]>=0.115.0->vllm)\n",
            "  Using cached fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from vllm) (3.11.15)\n",
            "Requirement already satisfied: openai>=1.52.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.76.2)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.11.4)\n",
            "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from vllm) (11.2.1)\n",
            "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\n",
            "  Using cached prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting tiktoken>=0.6.0 (from vllm)\n",
            "  Using cached tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting lm-format-enforcer<0.11,>=0.10.11 (from vllm)\n",
            "  Using cached lm_format_enforcer-0.10.11-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting llguidance<0.8.0,>=0.7.9 (from vllm)\n",
            "  Using cached llguidance-0.7.19-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting outlines==0.1.11 (from vllm)\n",
            "  Using cached outlines-0.1.11-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting lark==1.2.2 (from vllm)\n",
            "  Using cached lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xgrammar==0.1.18 (from vllm)\n",
            "  Using cached xgrammar-0.1.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.13.2)\n",
            "Collecting partial-json-parser (from vllm)\n",
            "  Using cached partial_json_parser-0.2.1.1.post5-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting pyzmq>=25.0.0 (from vllm)\n",
            "  Using cached pyzmq-26.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.0 kB)\n",
            "Collecting msgspec (from vllm)\n",
            "  Using cached msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting gguf>=0.13.0 (from vllm)\n",
            "  Using cached gguf-0.16.3-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from vllm) (8.7.0)\n",
            "Collecting mistral_common>=1.5.4 (from mistral_common[opencv]>=1.5.4->vllm)\n",
            "  Using cached mistral_common-1.5.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.11.0.86)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from vllm) (6.0.2)\n",
            "Collecting compressed-tensors==0.9.3 (from vllm)\n",
            "  Using cached compressed_tensors-0.9.3-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting depyf==0.18.0 (from vllm)\n",
            "  Using cached depyf-0.18.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from vllm) (3.1.1)\n",
            "Collecting watchfiles (from vllm)\n",
            "  Using cached watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting python-json-logger (from vllm)\n",
            "  Using cached python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from vllm) (1.15.2)\n",
            "Collecting opentelemetry-sdk<1.27.0,>=1.26.0 (from vllm)\n",
            "  Using cached opentelemetry_sdk-1.26.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api<1.27.0,>=1.26.0 (from vllm)\n",
            "  Using cached opentelemetry_api-1.26.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp<1.27.0,>=1.26.0 (from vllm)\n",
            "  Using cached opentelemetry_exporter_otlp-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-semantic-conventions-ai<0.5.0,>=0.4.1 (from vllm)\n",
            "  Using cached opentelemetry_semantic_conventions_ai-0.4.6-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting numba==0.61.2 (from vllm)\n",
            "  Using cached numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting ray!=2.44.*,>=2.43.0 (from ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n",
            "  Using cached ray-2.45.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: torchaudio==2.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision==0.21.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.0+cu124)\n",
            "Collecting xformers==0.0.29.post2 (from vllm)\n",
            "  Using cached xformers-0.0.29.post2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting astor (from depyf==0.18.0->vllm)\n",
            "  Using cached astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from depyf==0.18.0->vllm) (0.3.8)\n",
            "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm)\n",
            "  Using cached llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
            "Collecting interegular (from outlines==0.1.11->vllm)\n",
            "  Using cached interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (3.1.6)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (1.6.0)\n",
            "Collecting diskcache (from outlines==0.1.11->vllm)\n",
            "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: referencing in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.36.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (4.23.0)\n",
            "Collecting pycountry (from outlines==0.1.11->vllm)\n",
            "  Using cached pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting airportsdata (from outlines==0.1.11->vllm)\n",
            "  Using cached airportsdata-20250224-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting outlines_core==0.1.26 (from outlines==0.1.11->vllm)\n",
            "  Using cached outlines_core-0.1.26-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.0.5) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.0.5) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.0.5) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.0.5) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.0.5) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.0.5) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.0.5) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.0.5) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.0.5) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.0.5) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.0.5) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.0.5) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.0.5) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.0.5) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.0.5) (12.4.127)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch (from flash-attn==2.0.5)\n",
            "  Using cached torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.8.5-cp38-abi3-manylinux1_x86_64.whl.metadata (14 kB)\n",
            "Collecting torch (from flash-attn==2.0.5)\n",
            "  Using cached torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting sympy>=1.13.3 (from torch->flash-attn==2.0.5)\n",
            "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting torch (from flash-attn==2.0.5)\n",
            "  Using cached torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "  Using cached torch-2.5.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "  Using cached torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.0.5) (1.13.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "INFO: pip is still looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch (from flash-attn==2.0.5)\n",
            "  Using cached torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "  Using cached torch-2.3.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting torch (from flash-attn==2.0.5)\n",
            "  Using cached torch-2.3.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "  Using cached torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting torch (from flash-attn==2.0.5)\n",
            "  Using cached torch-2.2.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached torch-2.2.0-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "  Using cached torch-2.1.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting nvidia-nccl-cu12==2.18.1 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting torch (from flash-attn==2.0.5)\n",
            "  Using cached torch-2.1.1-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "  Using cached torch-2.1.0-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "  Using cached torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->flash-attn==2.0.5) (80.3.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->flash-attn==2.0.5) (0.45.1)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.8.4-cp38-abi3-manylinux1_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.11/dist-packages (from vllm) (24.0.1)\n",
            "  Using cached vllm-0.8.3-cp38-abi3-manylinux1_x86_64.whl.metadata (27 kB)\n",
            "Collecting xgrammar==0.1.17 (from vllm)\n",
            "  Using cached xgrammar-0.1.17-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting gguf==0.10.0 (from vllm)\n",
            "  Using cached gguf-0.10.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting compressed-tensors==0.9.2 (from vllm)\n",
            "  Using cached compressed_tensors-0.9.2-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting numba==0.61 (from vllm)\n",
            "  Using cached numba-0.61.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.8.2-cp38-abi3-manylinux1_x86_64.whl.metadata (27 kB)\n",
            "Collecting numpy<2.0.0 (from vllm)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting xgrammar==0.1.16 (from vllm)\n",
            "  Using cached xgrammar-0.1.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: numba==0.60.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.60.0)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.8.1-cp38-abi3-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "  Using cached vllm-0.8.0-cp38-abi3-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "  Using cached vllm-0.7.3-cp38-abi3-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting xgrammar==0.1.11 (from vllm)\n",
            "  Using cached xgrammar-0.1.11-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting compressed-tensors==0.9.1 (from vllm)\n",
            "  Using cached compressed_tensors-0.9.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting ray==2.40.0 (from ray[adag]==2.40.0->vllm)\n",
            "  Using cached ray-2.40.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (17 kB)\n",
            "Collecting torchaudio==2.5.1 (from vllm)\n",
            "  Using cached torchaudio-2.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting torchvision==0.20.1 (from vllm)\n",
            "  Using cached torchvision-0.20.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting xformers==0.0.28.post3 (from vllm)\n",
            "  Using cached xformers-0.0.28.post3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.7.2-cp38-abi3-manylinux1_x86_64.whl.metadata (12 kB)\n",
            "Collecting uvicorn[standard] (from vllm)\n",
            "  Using cached uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: nvidia-ml-py>=12.560.30 in /usr/local/lib/python3.11/dist-packages (from vllm) (12.570.86)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.7.1-cp38-abi3-manylinux1_x86_64.whl.metadata (12 kB)\n",
            "Collecting compressed-tensors==0.9.0 (from vllm)\n",
            "  Using cached compressed_tensors-0.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.7.0-cp38-abi3-manylinux1_x86_64.whl.metadata (12 kB)\n",
            "  Using cached vllm-0.6.6.post1-cp38-abi3-manylinux1_x86_64.whl.metadata (11 kB)\n",
            "Collecting compressed-tensors==0.8.1 (from vllm)\n",
            "  Using cached compressed_tensors-0.8.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.6.6-cp38-abi3-manylinux1_x86_64.whl.metadata (11 kB)\n",
            "  Using cached vllm-0.6.5-cp38-abi3-manylinux1_x86_64.whl.metadata (11 kB)\n",
            "  Using cached vllm-0.6.4.post1-cp38-abi3-manylinux1_x86_64.whl.metadata (10 kB)\n",
            "Collecting outlines<0.1,>=0.0.43 (from vllm)\n",
            "  Using cached outlines-0.0.46-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting compressed-tensors==0.8.0 (from vllm)\n",
            "  Using cached compressed_tensors-0.8.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.6.4-cp38-abi3-manylinux1_x86_64.whl.metadata (10 kB)\n",
            "Collecting lm-format-enforcer==0.10.6 (from vllm)\n",
            "  Using cached lm_format_enforcer-0.10.6-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.6.3.post1-cp38-abi3-manylinux1_x86_64.whl.metadata (10 kB)\n",
            "Collecting compressed-tensors==0.6.0 (from vllm)\n",
            "  Using cached compressed_tensors-0.6.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting torchvision==0.19 (from vllm)\n",
            "  Using cached torchvision-0.19.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "Collecting xformers==0.0.27.post2 (from vllm)\n",
            "  Using cached xformers-0.0.27.post2-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.6.3-cp38-abi3-manylinux1_x86_64.whl.metadata (10 kB)\n",
            "  Using cached vllm-0.6.2-cp38-abi3-manylinux1_x86_64.whl.metadata (2.4 kB)\n",
            "  Using cached vllm-0.6.1.post2-cp38-abi3-manylinux1_x86_64.whl.metadata (2.4 kB)\n",
            "Collecting gguf==0.9.1 (from vllm)\n",
            "  Using cached gguf-0.9.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting vllm-flash-attn==2.6.1 (from vllm)\n",
            "  Using cached vllm_flash_attn-2.6.1-cp311-cp311-manylinux1_x86_64.whl.metadata (476 bytes)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.6.1.post1-cp38-abi3-manylinux1_x86_64.whl.metadata (2.3 kB)\n",
            "  Using cached vllm-0.6.1-cp38-abi3-manylinux1_x86_64.whl.metadata (2.3 kB)\n",
            "  Using cached vllm-0.6.0-cp38-abi3-manylinux1_x86_64.whl.metadata (2.2 kB)\n",
            "  Using cached vllm-0.5.5-cp38-abi3-manylinux1_x86_64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from vllm) (0.11.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (from vllm) (0.13.1)\n",
            "  Using cached vllm-0.5.4-cp38-abi3-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting lm-format-enforcer==0.10.3 (from vllm)\n",
            "  Using cached lm_format_enforcer-0.10.3-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.5.3.post1-cp38-abi3-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting torchvision==0.18.1 (from vllm)\n",
            "  Using cached torchvision-0.18.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting xformers==0.0.27 (from vllm)\n",
            "  Using cached xformers-0.0.27-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting vllm-flash-attn==2.5.9.post1 (from vllm)\n",
            "  Using cached vllm_flash_attn-2.5.9.post1-cp311-cp311-manylinux1_x86_64.whl.metadata (482 bytes)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.5.3-cp38-abi3-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "  Using cached vllm-0.5.2-cp38-abi3-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "  Using cached vllm-0.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (8.2 kB)\n",
            "Collecting lm-format-enforcer==0.10.1 (from vllm)\n",
            "  Using cached lm_format_enforcer-0.10.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting outlines>=0.0.43 (from vllm)\n",
            "  Using cached outlines-0.2.3-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting torchvision==0.18.0 (from vllm)\n",
            "  Using cached torchvision-0.18.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting xformers==0.0.26.post1 (from vllm)\n",
            "  Using cached xformers-0.0.26.post1-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting vllm-flash-attn==2.5.9 (from vllm)\n",
            "  Using cached vllm_flash_attn-2.5.9-cp311-cp311-manylinux1_x86_64.whl.metadata (476 bytes)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.5.0.post1-cp311-cp311-manylinux1_x86_64.whl.metadata (8.1 kB)\n",
            "  Using cached vllm-0.5.0-cp311-cp311-manylinux1_x86_64.whl.metadata (8.3 kB)\n",
            "  Using cached vllm-0.4.3-cp311-cp311-manylinux1_x86_64.whl.metadata (7.8 kB)\n",
            "Collecting outlines==0.0.34 (from vllm)\n",
            "  Using cached outlines-0.0.34-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting vllm-flash-attn==2.5.8.post2 (from vllm)\n",
            "  Using cached vllm_flash_attn-2.5.8.post2-cp311-cp311-manylinux1_x86_64.whl.metadata (482 bytes)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.4.2-cp311-cp311-manylinux1_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting tiktoken==0.6.0 (from vllm)\n",
            "  Using cached tiktoken-0.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting lm-format-enforcer==0.9.8 (from vllm)\n",
            "  Using cached lm_format_enforcer-0.9.8-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting vllm-nccl-cu12<2.19,>=2.18 (from vllm)\n",
            "  Using cached vllm_nccl_cu12-2.18.1.0.4.0.tar.gz (6.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.4.1-cp311-cp311-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
            "Collecting xformers==0.0.25 (from vllm)\n",
            "  Using cached xformers-0.0.25-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.4.0.post1-cp311-cp311-manylinux1_x86_64.whl.metadata (8.6 kB)\n",
            "Collecting xformers==0.0.23.post1 (from vllm)\n",
            "  Using cached xformers-0.0.23.post1-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting pynvml==11.5.0 (from vllm)\n",
            "  Using cached pynvml-11.5.0-py3-none-any.whl.metadata (7.8 kB)\n",
            "INFO: pip is looking at multiple versions of vllm to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (8.6 kB)\n",
            "  Using cached vllm-0.3.3-cp311-cp311-manylinux1_x86_64.whl.metadata (7.8 kB)\n",
            "  Using cached vllm-0.3.2-cp311-cp311-manylinux1_x86_64.whl.metadata (7.5 kB)\n",
            "Collecting aioprometheus[starlette] (from vllm)\n",
            "  Using cached aioprometheus-23.12.0-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.3.1-cp311-cp311-manylinux1_x86_64.whl.metadata (7.4 kB)\n",
            "  Using cached vllm-0.3.0-cp311-cp311-manylinux1_x86_64.whl.metadata (7.4 kB)\n",
            "INFO: pip is still looking at multiple versions of vllm to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached vllm-0.2.7-cp311-cp311-manylinux1_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting pydantic==1.10.13 (from vllm)\n",
            "  Using cached pydantic-1.10.13-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (149 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.2.6-cp311-cp311-manylinux1_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from vllm) (2.2.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from vllm) (18.1.0)\n",
            "  Using cached vllm-0.2.5-cp311-cp311-manylinux1_x86_64.whl.metadata (6.5 kB)\n",
            "Collecting xformers>=0.0.23 (from vllm)\n",
            "  Using cached xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.2.4-cp311-cp311-manylinux1_x86_64.whl.metadata (6.8 kB)\n",
            "  Using cached vllm-0.2.3-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "  Using cached vllm-0.2.2-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "  Using cached vllm-0.2.1.post1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.2 kB)\n",
            "Collecting xformers==0.0.22 (from vllm)\n",
            "  Using cached xformers-0.0.22-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting pydantic<2 (from vllm)\n",
            "  Using cached pydantic-1.10.22-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (154 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (8.1.8)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.51.1->vllm) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.51.1->vllm) (0.5.3)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm)\n",
            "  Using cached starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->outlines==0.1.11->vllm) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (2025.4.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (0.24.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->vllm) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->vllm) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->vllm) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->vllm) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->flash-attn==2.0.5) (1.3.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm) (0.16.0)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]->vllm)\n",
            "  Using cached httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]->vllm)\n",
            "  Using cached python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]->vllm)\n",
            "  Using cached uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm) (15.0.1)\n",
            "Using cached triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "Using cached torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\n",
            "Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "Using cached nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "Using cached nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "Using cached nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "Using cached nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "Using cached nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "Using cached nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "Using cached vllm-0.2.1.post1-cp311-cp311-manylinux1_x86_64.whl (28.7 MB)\n",
            "Using cached xformers-0.0.22-cp311-cp311-manylinux2014_x86_64.whl (211.6 MB)\n",
            "Using cached pydantic-1.10.22-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "Using cached ray-2.45.0-cp311-cp311-manylinux2014_x86_64.whl (68.4 MB)\n",
            "Using cached fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "Using cached starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "Using cached lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "Using cached uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "Using cached httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "Using cached python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Using cached uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "Using cached watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n",
            "Building wheels for collected packages: flash-attn\n",
            "\u001b[33m  DEPRECATION: Building 'flash-attn' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'flash-attn'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install \\\n",
        "    flash-attn==2.0.5 \\\n",
        "    triton==2.0.0 \\\n",
        "    vllm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPZVbDIHPQUc",
        "outputId": "11423099-886f-46bb-a2fa-27e5f2240fbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjy3475\u001b[0m (\u001b[33mHPML-Energy-Efficient-LLM\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IbU-A3ARw1k",
        "outputId": "3d4567ee-17ae-493e-bd4c-21c73c96d196"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mCurrent Settings\u001b[0m\n",
            "{\n",
            "  \"_extra_http_headers\": null,\n",
            "  \"_proxies\": null,\n",
            "  \"api_key\": null,\n",
            "  \"base_url\": \"https://api.wandb.ai\",\n",
            "  \"entity\": null,\n",
            "  \"git_remote\": \"origin\",\n",
            "  \"ignore_globs\": [],\n",
            "  \"organization\": null,\n",
            "  \"project\": null,\n",
            "  \"root_dir\": null,\n",
            "  \"section\": \"default\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "!wandb status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fy6rvCvQSQcz"
      },
      "outputs": [],
      "source": [
        "# import wandb\n",
        "\n",
        "# wandb.init(project=\"HPML-Energy-Efficient-LLM\", name=\"test-connection\")\n",
        "\n",
        "# wandb.log({\"test_value\": \"init wandb\"})\n",
        "\n",
        "# wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "YVOgToaUjT50"
      },
      "outputs": [],
      "source": [
        "# global configuration for experiments\n",
        "cfg = {\n",
        "    \"task\":           \"math\",\n",
        "    \"model\":          \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
        "    # test adaptive quant\n",
        "    \"modes\": [\n",
        "        \"adaptive\"\n",
        "        # \"fp32_vanilla\",    # FP16 + vanilla Transformer\n",
        "        # \"fp16_vanilla\",    # FP16 + vanilla Transformer\n",
        "        # \"int8_vanilla\",    # INT8 + vanilla\n",
        "        # \"int4_vanilla\",    # INT4 + vanilla&\n",
        "    ],\n",
        "    # for adaptive mode: which two modes to switch between\n",
        "    \"high_mode\":      \"fp16_vanilla\",\n",
        "    \"low_mode\":       \"int8_vanilla\",\n",
        "    \"ctx_threshold\": 512,\n",
        "    \"latency_threshold\": 0.08,\n",
        "\n",
        "    \"dataset_name\":   \"deepmind/math_dataset\",\n",
        "    \"dataset_config\": \"algebra__linear_1d\",\n",
        "    \"split\":          \"test\",\n",
        "    \"num_examples\":   20,\n",
        "\n",
        "    \"temperature\": 0.5,\n",
        "    \"top_p\":0.9,\n",
        "    \"verbose\":        True,\n",
        "    \"output_file\":    \"results.json\",\n",
        "    \"device_map\": \"cuda\"\n",
        "\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "r5e1v5JwvDfu"
      },
      "outputs": [],
      "source": [
        "# global configuration for experiments\n",
        "cfg = {\n",
        "    \"task\":           \"math\",\n",
        "    \"model\":          \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
        "    # test default (vanilla) kernel\n",
        "    \"modes\": [\n",
        "        \"fp32_vanilla\",    # FP16 + vanilla Transformer\n",
        "        \"fp16_vanilla\",    # FP16 + vanilla Transformer\n",
        "        \"int8_vanilla\",    # INT8 + vanilla\n",
        "        \"int4_vanilla\",    # INT4 + vanilla&\n",
        "    ],\n",
        "\n",
        "    \"dataset_name\":   \"deepmind/math_dataset\",\n",
        "    \"dataset_config\": \"algebra__linear_1d\",\n",
        "    \"split\":          \"test\",\n",
        "    \"num_examples\":   20,\n",
        "\n",
        "    \"temperature\": 0.5,\n",
        "    \"top_p\":0.9,\n",
        "    \"verbose\":        True,\n",
        "    \"output_file\":    \"results.json\",\n",
        "    \"device_map\": \"cuda\"\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9wki5wqKQHK4"
      },
      "outputs": [],
      "source": [
        "# exp for mbpp\n",
        "cfg = {\n",
        "    \"task\":           \"mbpp\",\n",
        "    # \"model\":          \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
        "    \"model\":       \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "    # test default (vanilla) kernel at fp16/int8/int4, plus adaptive switching\n",
        "    \"modes\":          [\n",
        "        # \"fp16_flash-v2\",    # FP16 + flash attn v2 Transformer\n",
        "        \"fp16\",\n",
        "        \"int8_vanilla\",    # INT8 + vanilla\n",
        "        \"int4_vanilla\",    # INT4 + vanilla& low_mode\n",
        "    ],\n",
        "    # for adaptive mode: which two modes to switch between\n",
        "    # \"high_mode\":      \"fp16_vanilla\",\n",
        "    # \"low_mode\":       \"int8_vanilla\",\n",
        "    \"temperature\": 0.5,\n",
        "    \"top_p\":0.9,\n",
        "    \"num_examples\":   500,\n",
        "    \"quick\":          True,\n",
        "    \"max_samples\":    500,\n",
        "    \"batch_size\":     8,\n",
        "    \"verbose\":        True,\n",
        "    \"output_file\":    \"mbpp_results.json\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yA9v4noz0XtJ",
        "outputId": "82dd5385-0c07-4304-d6f4-0da250ad0084"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/vLLM/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/opt/rocm/lib/libamd_smi.so: cannot open shared object file: No such file or directory\n",
            "Unable to find libamd_smi.so library try installing amd-smi-lib from your package manager\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset  # ensure load_dataset is defined\n",
        "\n",
        "# benchmark functions\n",
        "from utils.test_generation import quick_test_generation, test_generation_MATH, test_generation_MBPP\n",
        "from utils.test_mmlu    import quick_test_mmlu, test_quantized_models_on_mmlu\n",
        "from utils.test_glue    import test_quantized_models_on_glue\n",
        "\n",
        "# energy & tracking\n",
        "from utils.energy_utils   import EnergyTracker, get_carbon_intensity, joules_to_co2\n",
        "from utils.memory_utils   import clean_memory\n",
        "\n",
        "# adaptive quant wrapper\n",
        "from utils.adaptive_quant  import AdaptiveQuantGenerator\n",
        "\n",
        "# plotting\n",
        "from utils.plot_utils    import plot_energy_comparison, plot_component_energy\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1oM_9fRa1Qq5"
      },
      "outputs": [],
      "source": [
        "# Monkey-patch EnergyTracker to support `with tracker:` and save_results\n",
        "def _et_enter(self):\n",
        "    if getattr(self, 'zeus', None):\n",
        "        try:\n",
        "            self.zeus.begin_window('inference')\n",
        "            self.active_windows.add('inference')\n",
        "        except:\n",
        "            pass\n",
        "    self._enter_ts = time.time()\n",
        "    return self\n",
        "\n",
        "def _et_exit(self, exc_type, exc_val, exc_tb):\n",
        "    end_ts = time.time()\n",
        "    inf_e = 0\n",
        "    if getattr(self, 'zeus', None) and 'inference' in self.active_windows:\n",
        "        try:\n",
        "            m = self.zeus.end_window('inference')\n",
        "            inf_e = m.total_energy\n",
        "            self.active_windows.remove('inference')\n",
        "        except:\n",
        "            pass\n",
        "    elapsed = end_ts - getattr(self, '_enter_ts', end_ts)\n",
        "    comp = {k: np.sum(v) for k, v in self.comp_energy.items()}\n",
        "    self.stats = {\n",
        "        'total_energy': inf_e,\n",
        "        'time': elapsed,\n",
        "        'components': comp,\n",
        "        'num_tokens': None\n",
        "    }\n",
        "    return False\n",
        "\n",
        "def _save_results(self, extra_metrics):\n",
        "    if not hasattr(self, 'stats'):\n",
        "        self.stats = {}\n",
        "    self.stats.update(extra_metrics)\n",
        "\n",
        "EnergyTracker.__enter__    = _et_enter\n",
        "EnergyTracker.__exit__     = _et_exit\n",
        "EnergyTracker.save_results = _save_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BgCZi3UfjVih"
      },
      "outputs": [],
      "source": [
        "def run_task(cfg):\n",
        "    \"\"\"Dispatch benchmarks based on cfg['task'], with adaptive-quant support.\"\"\"\n",
        "    task = cfg[\"task\"]\n",
        "    modes = list(cfg[\"modes\"])\n",
        "    results = {}\n",
        "\n",
        "    # unpack shared params\n",
        "    high_mode  = cfg.get(\"high_mode\")\n",
        "    low_mode   = cfg.get(\"low_mode\")\n",
        "    ctx_th     = cfg.get(\"ctx_threshold\", 1024)\n",
        "    lat_th     = cfg.get(\"latency_threshold\", 0.08)\n",
        "    device_map = cfg.get(\"device_map\", \"auto\")\n",
        "    temp       = cfg.get(\"temperature\", 0.5)\n",
        "    top_p      = cfg.get(\"top_p\", 0.9)\n",
        "    carbon_int = get_carbon_intensity()\n",
        "\n",
        "    # skip adaptive for pure classification\n",
        "    if task in (\"glue\", \"mmlu\") and \"adaptive\" in modes:\n",
        "        print(\"⚠️  Skipping adaptive for classification tasks\")\n",
        "        modes.remove(\"adaptive\")\n",
        "\n",
        "    # -------------------------\n",
        "    # TEXT GENERATION\n",
        "    # -------------------------\n",
        "    if task == \"generation\":\n",
        "        results[\"generation\"] = {}\n",
        "\n",
        "        # adaptive generation\n",
        "        if \"adaptive\" in modes:\n",
        "            print(\"\\n=== ADAPTIVE generation ===\")\n",
        "            agent = AdaptiveQuantGenerator(\n",
        "                cfg[\"model\"],\n",
        "                high_mode=high_mode,\n",
        "                low_mode=low_mode,\n",
        "                ctx_threshold=ctx_th,\n",
        "                latency_threshold=lat_th,\n",
        "                device_map=device_map\n",
        "            )\n",
        "            _, _, stats = agent.generate(\n",
        "                cfg[\"prompt\"],\n",
        "                max_new_tokens=cfg[\"tokens\"],\n",
        "                temperature=temp,\n",
        "                top_p=top_p\n",
        "            )\n",
        "            results[\"generation\"][\"adaptive\"] = stats\n",
        "            modes.remove(\"adaptive\")\n",
        "\n",
        "        # static generation modes\n",
        "        for mode in modes:\n",
        "            print(f\"\\n=== {mode.upper()} generation ===\")\n",
        "            stats = quick_test_generation(\n",
        "                model_name=cfg[\"model\"],\n",
        "                quant_mode=mode,\n",
        "                prompt=cfg[\"prompt\"],\n",
        "                max_new_tokens=cfg[\"tokens\"],\n",
        "                temperature=temp,\n",
        "                top_p=top_p\n",
        "            )\n",
        "            results[\"generation\"][mode] = stats\n",
        "\n",
        "    # -------------------------\n",
        "    # MATH (generation-style)\n",
        "    # -------------------------\n",
        "    elif task == \"math\":\n",
        "        results[\"math\"] = {}\n",
        "\n",
        "        # adaptive MATH\n",
        "        if \"adaptive\" in modes:\n",
        "            print(\"\\n=== ADAPTIVE on MATH ===\")\n",
        "            ds = load_dataset(\n",
        "                cfg[\"dataset_name\"],\n",
        "                cfg[\"dataset_config\"],\n",
        "                split=cfg[\"split\"]\n",
        "            ).select(range(cfg[\"num_examples\"]))\n",
        "\n",
        "            agent = AdaptiveQuantGenerator(\n",
        "                cfg[\"model\"],\n",
        "                high_mode=high_mode,\n",
        "                low_mode=low_mode,\n",
        "                ctx_threshold=ctx_th,\n",
        "                latency_threshold=lat_th,\n",
        "                device_map=device_map\n",
        "            )\n",
        "\n",
        "            examples, correct, total_tokens = [], 0, 0\n",
        "            for ex in tqdm(ds, desc=\"Adaptive MATH\"):\n",
        "                prompt = f\"<｜User｜>{ex['question']}<｜Assistant｜><think>\"\n",
        "                gen_ids, _, stats = agent.evaluate(\n",
        "                    prompt,\n",
        "                    agent.tokenizer,\n",
        "                    max_new_tokens=cfg.get(\"max_new_tokens\", 32),\n",
        "                    temperature=temp,\n",
        "                    top_p=top_p\n",
        "                )\n",
        "                inp_len = stats[\"input_length\"]\n",
        "                pred = agent.tokenizer.decode(\n",
        "                    gen_ids[0, inp_len:], skip_special_tokens=True\n",
        "                ).strip()\n",
        "\n",
        "                is_corr = (pred == ex[\"answer\"].strip())\n",
        "                correct += int(is_corr)\n",
        "                total_tokens += stats.get(\"num_tokens\", 1)\n",
        "                examples.append({\n",
        "                    \"question\":   ex[\"question\"],\n",
        "                    \"prediction\": pred,\n",
        "                    \"is_correct\": is_corr,\n",
        "                    \"stats\":      stats\n",
        "                })\n",
        "                clean_memory()\n",
        "\n",
        "            n = len(examples)\n",
        "            total_e = sum(e[\"stats\"][\"total_energy\"] for e in examples)\n",
        "            total_t = sum(e[\"stats\"][\"time\"]         for e in examples)\n",
        "            results[\"math\"][\"adaptive\"] = {\n",
        "                \"examples\": examples,\n",
        "                \"summary\": {\n",
        "                    \"accuracy\":         100 * correct / n,\n",
        "                    \"avg_energy\":       total_e / n,\n",
        "                    \"avg_time\":         total_t / n,\n",
        "                    \"energy_per_token\": total_e / total_tokens,\n",
        "                    \"carbon_emissions\": joules_to_co2(total_e, carbon_int)\n",
        "                }\n",
        "            }\n",
        "            plot_component_energy(results, task_type=\"math\", quant_mode=\"adaptive\")\n",
        "            modes.remove(\"adaptive\")\n",
        "\n",
        "        # static MATH modes\n",
        "        if modes:\n",
        "            print(f\"\\n=== standard modes on MATH: {modes} ===\")\n",
        "            std = test_generation_MATH(\n",
        "                model_name=cfg[\"model\"],\n",
        "                quantization_modes=modes,\n",
        "                dataset_name=cfg[\"dataset_name\"],\n",
        "                dataset_config=cfg[\"dataset_config\"],\n",
        "                split=cfg[\"split\"],\n",
        "                num_examples=cfg[\"num_examples\"],\n",
        "                verbose=cfg.get(\"verbose\", True)\n",
        "            )\n",
        "            results[\"math\"].update(std)\n",
        "\n",
        "    # -------------------------\n",
        "    # MBPP\n",
        "    # -------------------------\n",
        "    elif task == \"mbpp\":\n",
        "        print(\"\\n=== MBPP task ===\")\n",
        "        results[\"mbpp\"] = {}\n",
        "\n",
        "        # adaptive MBPP\n",
        "        if \"adaptive\" in modes:\n",
        "            print(\"\\n=== ADAPTIVE on MBPP ===\")\n",
        "            ds = load_dataset(\n",
        "                cfg[\"dataset_name\"],\n",
        "                split=cfg.get(\"split\", \"test\")\n",
        "            ).select(range(cfg[\"num_examples\"]))\n",
        "\n",
        "            agent = AdaptiveQuantGenerator(\n",
        "                cfg[\"model\"],\n",
        "                high_mode=high_mode,\n",
        "                low_mode=low_mode,\n",
        "                ctx_threshold=ctx_th,\n",
        "                latency_threshold=lat_th,\n",
        "                device_map=device_map\n",
        "            )\n",
        "\n",
        "            examples, correct, total_tokens = [], 0, 0\n",
        "            for ex in tqdm(ds, desc=\"Adaptive MBPP\"):\n",
        "                prompt = ex[\"text\"]\n",
        "                gen_ids, _, stats = agent.evaluate(\n",
        "                    prompt,\n",
        "                    agent.tokenizer,\n",
        "                    max_new_tokens=cfg.get(\"max_new_tokens\", 128),\n",
        "                    temperature=temp,\n",
        "                    top_p=top_p\n",
        "                )\n",
        "                inp_len = stats[\"input_length\"]\n",
        "                pred = agent.tokenizer.decode(\n",
        "                    gen_ids[0, inp_len:], skip_special_tokens=True\n",
        "                )\n",
        "                is_corr = check_mbpp(pred, ex[\"expected_code\"])\n",
        "                correct += int(is_corr)\n",
        "                total_tokens += stats.get(\"num_tokens\", 1)\n",
        "                examples.append({\n",
        "                    \"prompt\":     prompt,\n",
        "                    \"prediction\": pred,\n",
        "                    \"is_correct\": is_corr,\n",
        "                    \"stats\":      stats\n",
        "                })\n",
        "                clean_memory()\n",
        "\n",
        "            n = len(examples)\n",
        "            total_e = sum(e[\"stats\"][\"total_energy\"] for e in examples)\n",
        "            total_t = sum(e[\"stats\"][\"time\"]         for e in examples)\n",
        "            results[\"mbpp\"][\"adaptive\"] = {\n",
        "                \"examples\": examples,\n",
        "                \"summary\": {\n",
        "                    \"accuracy\":         100 * correct / n,\n",
        "                    \"avg_energy\":       total_e / n,\n",
        "                    \"avg_time\":         total_t / n,\n",
        "                    \"energy_per_token\": total_e / total_tokens,\n",
        "                    \"carbon_emissions\": joules_to_co2(total_e, carbon_int)\n",
        "                }\n",
        "            }\n",
        "            modes.remove(\"adaptive\")\n",
        "\n",
        "        # static MBPP modes\n",
        "        std = test_generation_MBPP(\n",
        "            model_name=cfg[\"model\"],\n",
        "            quantization_modes=modes,\n",
        "            num_examples=cfg[\"num_examples\"],\n",
        "            verbose=cfg.get(\"verbose\", True),\n",
        "            temperature=temp,\n",
        "            top_p=top_p\n",
        "        )\n",
        "        results[\"mbpp\"].update(std)\n",
        "\n",
        "    # -------------------------\n",
        "    # MMLU\n",
        "    # -------------------------\n",
        "    elif task == \"mmlu\":\n",
        "        print(\"\\n=== MMLU task ===\")\n",
        "\n",
        "        if cfg.get(\"quick\", False):\n",
        "            stats = quick_test_mmlu(\n",
        "                model_name=cfg[\"model\"],\n",
        "                quant_mode=modes[0],\n",
        "                subjects=cfg[\"subjects\"],\n",
        "                max_samples=cfg[\"max_samples\"]\n",
        "            )\n",
        "        else:\n",
        "            stats = test_quantized_models_on_mmlu(\n",
        "                model_name=cfg[\"model\"],\n",
        "                quantization_modes=modes,\n",
        "                subjects=cfg[\"subjects\"]\n",
        "            )\n",
        "        results[\"mmlu\"] = stats\n",
        "\n",
        "    # -------------------------\n",
        "    # GLUE\n",
        "    # -------------------------\n",
        "    else:\n",
        "        print(\"\\n=== GLUE task ===\")\n",
        "        results[\"glue\"] = test_quantized_models_on_glue(\n",
        "            model_name=cfg[\"model\"],\n",
        "            tasks=cfg[\"glue_tasks\"],\n",
        "            quantization_modes=modes,\n",
        "            batch_size=cfg[\"batch_size\"]\n",
        "        )\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvX3mdfXa5P6",
        "outputId": "04ad6dbc-b1a3-425c-da1e-4fbfa4f72aa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Location detected: Las Vegas, US (lat: 36.175, lon: -115.1372)\n",
            "Using estimated carbon intensity.\n",
            "Using estimate for US: 417 gCO2eq/kWh\n",
            "\n",
            "=== standard modes on MATH: ['fp32_vanilla', 'fp16_vanilla', 'int8_vanilla', 'int4_vanilla'] ===\n",
            "Location detected: Las Vegas, US (lat: 36.175, lon: -115.1372)\n",
            "Using estimated carbon intensity.\n",
            "Using estimate for US: 417 gCO2eq/kWh\n",
            "Carbon intensity: 417 gCO2eq/kWh\n",
            "\n",
            "=== Testing FP32_VANILLA on MATH ===\n",
            "Loading FP32 model …\n",
            "GPU Memory: Allocated: 0.01 GB | Reserved: 0.02 GB | Max: 4.21 GB\n",
            "Model ready → quantisation: FP32, kernel: vanilla\n",
            "GPU Memory: Allocated: 5.40 GB | Reserved: 5.55 GB | Max: 5.40 GB\n",
            "[2025-05-07 07:41:22,008] [zeus.monitor.energy](energy.py:209) Monitoring GPU indices [0].\n",
            "[2025-05-07 07:41:22,009] [zeus.monitor.energy](energy.py:210) Monitoring CPU indices []\n",
            "Successfully initialized ZeusMonitor\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "MATH FP32_VANILLA: 100%|██████████| 20/20 [00:09<00:00,  2.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FP32_VANILLA SUMMARY: Samples=20, Acc=0.00%,\n",
            "\n",
            "=== Testing FP16_VANILLA on MATH ===\n",
            "Loading FP16 model …\n",
            "GPU Memory: Allocated: 0.01 GB | Reserved: 5.41 GB | Max: 5.43 GB\n",
            "Model ready → quantisation: FP16, kernel: vanilla\n",
            "GPU Memory: Allocated: 2.71 GB | Reserved: 5.41 GB | Max: 5.43 GB\n",
            "[2025-05-07 07:41:34,691] [zeus.monitor.energy](energy.py:209) Monitoring GPU indices [0].\n",
            "[2025-05-07 07:41:34,691] [zeus.monitor.energy](energy.py:210) Monitoring CPU indices []\n",
            "Successfully initialized ZeusMonitor\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "MATH FP16_VANILLA: 100%|██████████| 20/20 [00:10<00:00,  1.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FP16_VANILLA SUMMARY: Samples=20, Acc=0.00%,\n",
            "\n",
            "=== Testing INT8_VANILLA on MATH ===\n",
            "Loading INT8 model …\n",
            "GPU Memory: Allocated: 0.01 GB | Reserved: 5.41 GB | Max: 5.43 GB\n",
            "Model ready → quantisation: INT8, kernel: vanilla\n",
            "GPU Memory: Allocated: 1.49 GB | Reserved: 5.41 GB | Max: 5.43 GB\n",
            "[2025-05-07 07:41:49,209] [zeus.monitor.energy](energy.py:209) Monitoring GPU indices [0].\n",
            "[2025-05-07 07:41:49,209] [zeus.monitor.energy](energy.py:210) Monitoring CPU indices []\n",
            "Successfully initialized ZeusMonitor\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rMATH INT8_VANILLA:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "MATH INT8_VANILLA: 100%|██████████| 20/20 [00:10<00:00,  2.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INT8_VANILLA SUMMARY: Samples=20, Acc=0.00%,\n",
            "\n",
            "=== Testing INT4_VANILLA on MATH ===\n",
            "Loading INT4 model …\n",
            "GPU Memory: Allocated: 0.01 GB | Reserved: 5.41 GB | Max: 5.43 GB\n",
            "Model ready → quantisation: INT4, kernel: vanilla\n",
            "GPU Memory: Allocated: 0.91 GB | Reserved: 5.43 GB | Max: 5.43 GB\n",
            "[2025-05-07 07:42:02,940] [zeus.monitor.energy](energy.py:209) Monitoring GPU indices [0].\n",
            "[2025-05-07 07:42:02,941] [zeus.monitor.energy](energy.py:210) Monitoring CPU indices []\n",
            "Successfully initialized ZeusMonitor\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "MATH INT4_VANILLA: 100%|██████████| 20/20 [00:11<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INT4_VANILLA SUMMARY: Samples=20, Acc=0.00%,\n"
          ]
        }
      ],
      "source": [
        "# results = run_task(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "Z5NwsmSuTlcb",
        "outputId": "a8a63bcd-5b0a-4c6a-e1ca-80bb81ea8b8a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B-mbpp-(fp16int8_vanilla/int4_vanilla)</strong> at: <a href='https://wandb.ai/fc2795-columbia-university/HPML-Energy-Efficient-LLM/runs/r4ne596a' target=\"_blank\">https://wandb.ai/fc2795-columbia-university/HPML-Energy-Efficient-LLM/runs/r4ne596a</a><br> View project at: <a href='https://wandb.ai/fc2795-columbia-university/HPML-Energy-Efficient-LLM' target=\"_blank\">https://wandb.ai/fc2795-columbia-university/HPML-Energy-Efficient-LLM</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250507_171233-r4ne596a/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/fc2795/HPML-Energy-Efficient-LLM/wandb/run-20250507_171319-y84rq6sv</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/fc2795-columbia-university/HPML-Energy-Efficient-LLM/runs/y84rq6sv' target=\"_blank\">deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B-mbpp-(fp16/int8_vanilla/int4_vanilla)</a></strong> to <a href='https://wandb.ai/fc2795-columbia-university/HPML-Energy-Efficient-LLM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/fc2795-columbia-university/HPML-Energy-Efficient-LLM' target=\"_blank\">https://wandb.ai/fc2795-columbia-university/HPML-Energy-Efficient-LLM</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/fc2795-columbia-university/HPML-Energy-Efficient-LLM/runs/y84rq6sv' target=\"_blank\">https://wandb.ai/fc2795-columbia-university/HPML-Energy-Efficient-LLM/runs/y84rq6sv</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Location detected: São Paulo, BR (lat: -23.5475, lon: -46.6361)\n",
            "Using estimated carbon intensity.\n",
            "Using estimate for BR: 110 gCO2eq/kWh\n",
            "\n",
            "=== MBPP task ===\n",
            "Location detected: São Paulo, BR (lat: -23.5475, lon: -46.6361)\n",
            "Using estimated carbon intensity.\n",
            "Using estimate for BR: 110 gCO2eq/kWh\n",
            "Carbon intensity: 110 gCO2eq/kWh\n",
            "\n",
            "=== Testing FP16 on MBPP ===\n",
            "Loading FP16 model …\n",
            "GPU Memory: Allocated: 1.15 GB | Reserved: 1.79 GB | Max: 1.78 GB\n",
            "Model ready → quantisation: FP16, kernel: vanilla\n",
            "GPU Memory: Allocated: 4.71 GB | Reserved: 5.40 GB | Max: 4.72 GB\n",
            "[2025-05-07 17:13:38,340] [zeus.device.gpu.nvidia](nvidia.py:47) pynvml is available and initialized.\n",
            "[2025-05-07 17:13:38,343] [zeus.device.cpu.rapl](rapl.py:137) RAPL is not supported on this CPU.\n",
            "[2025-05-07 17:13:38,344] [zeus.monitor.energy](energy.py:209) Monitoring GPU indices [0].\n",
            "[2025-05-07 17:13:38,345] [zeus.monitor.energy](energy.py:210) Monitoring CPU indices []\n",
            "Successfully initialized ZeusMonitor\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "MBPP FP16:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-05-07 17:13:38,482] [zeus.utils.framework](framework.py:25) PyTorch with CUDA support is available.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "MBPP FP16: 100%|██████████| 10/10 [00:07<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Testing INT8_VANILLA on MBPP ===\n",
            "Loading INT8 model …\n",
            "GPU Memory: Allocated: 1.18 GB | Reserved: 5.35 GB | Max: 5.69 GB\n",
            "Model ready → quantisation: INT8, kernel: vanilla\n",
            "GPU Memory: Allocated: 3.43 GB | Reserved: 5.37 GB | Max: 5.69 GB\n",
            "[2025-05-07 17:14:00,194] [zeus.monitor.energy](energy.py:209) Monitoring GPU indices [0].\n",
            "[2025-05-07 17:14:00,201] [zeus.monitor.energy](energy.py:210) Monitoring CPU indices []\n",
            "Successfully initialized ZeusMonitor\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "MBPP INT8_VANILLA:   0%|          | 0/10 [00:00<?, ?it/s]/opt/conda/envs/vLLM/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "MBPP INT8_VANILLA: 100%|██████████| 10/10 [00:07<00:00,  1.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Testing INT4_VANILLA on MBPP ===\n",
            "Loading INT4 model …\n",
            "GPU Memory: Allocated: 1.18 GB | Reserved: 5.35 GB | Max: 5.69 GB\n",
            "Model ready → quantisation: INT4, kernel: vanilla\n",
            "GPU Memory: Allocated: 2.80 GB | Reserved: 5.38 GB | Max: 5.69 GB\n",
            "[2025-05-07 17:14:30,376] [zeus.monitor.energy](energy.py:209) Monitoring GPU indices [0].\n",
            "[2025-05-07 17:14:30,377] [zeus.monitor.energy](energy.py:210) Monitoring CPU indices []\n",
            "Successfully initialized ZeusMonitor\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "MBPP INT4_VANILLA: 100%|██████████| 10/10 [00:06<00:00,  1.48it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B-mbpp-(fp16/int8_vanilla/int4_vanilla)</strong> at: <a href='https://wandb.ai/fc2795-columbia-university/HPML-Energy-Efficient-LLM/runs/y84rq6sv' target=\"_blank\">https://wandb.ai/fc2795-columbia-university/HPML-Energy-Efficient-LLM/runs/y84rq6sv</a><br> View project at: <a href='https://wandb.ai/fc2795-columbia-university/HPML-Energy-Efficient-LLM' target=\"_blank\">https://wandb.ai/fc2795-columbia-university/HPML-Energy-Efficient-LLM</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250507_171319-y84rq6sv/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Saved results to wandb\n"
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.init(\n",
        "    project=\"HPML-Energy-Efficient-LLM\",\n",
        "    name=f\"{cfg['model']}-{cfg['task']}-({'/'.join(cfg['modes'])})\",\n",
        "    tags=[cfg['model'].split('/')[-1], cfg['task']] + cfg['modes'],\n",
        "    group=cfg['model'].split('/')[-1],\n",
        "    job_type=cfg['task'],\n",
        "    config=cfg\n",
        ")\n",
        "\n",
        "results = run_task(cfg)\n",
        "\n",
        "wandb.log(results)\n",
        "wandb.finish()\n",
        "print(f\"\\nSaved results to wandb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ngjgsNnYhO3",
        "outputId": "a3677392-6ce4-422e-b496-a3e223d1b505"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'prompt': '\\n                <｜begin▁of▁sentence｜><｜User｜>output only the code, no explanation: Write a python function to remove first and last occurrence of a given character from the string.<｜Assistant｜><think>\\n                ',\n",
              " 'ground_truth_code': 'def remove_Occ(s,ch): \\r\\n    for i in range(len(s)): \\r\\n        if (s[i] == ch): \\r\\n            s = s[0 : i] + s[i + 1:] \\r\\n            break\\r\\n    for i in range(len(s) - 1,-1,-1):  \\r\\n        if (s[i] == ch): \\r\\n            s = s[0 : i] + s[i + 1:] \\r\\n            break\\r\\n    return s ',\n",
              " 'generated_code': ')\\n\\n</think> )\\n\\n\\n\\n\\n\\n\\n the number.\\n\\n no explanation\\n\\n  a function function that compute all occurrence last character of a character number in a string.\\n\\n The<think>\\nOkay First',\n",
              " 'test_cases': ['assert remove_Occ(\"hello\",\"l\") == \"heo\"',\n",
              "  'assert remove_Occ(\"abcda\",\"a\") == \"bcd\"',\n",
              "  'assert remove_Occ(\"PHP\",\"P\") == \"H\"'],\n",
              " 'is_correct': False,\n",
              " 'stats': {'total_energy': 30.890284558775836,\n",
              "  'tokenization_energy': 0.16228455877304077,\n",
              "  'inference_energy': 30.728000000002794,\n",
              "  'energy_per_token': 0.8825795588221668,\n",
              "  'time': 0.8039395809173584,\n",
              "  'components': {'embeddings': np.float64(0.4652470278739929),\n",
              "   'attention': np.float64(8.50546296334121),\n",
              "   'ffn': np.float64(11.028494139677496),\n",
              "   'layernorm': np.float64(0.3159266157150269),\n",
              "   'output_layer': np.float64(4.536999999996624)},\n",
              "  'num_tokens': 35}}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results[\"mbpp\"][\"fp16\"][\"examples\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "CHcEn3-pQHK6",
        "outputId": "4b5d1517-e0bd-4d73-a090-1a8106448718"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'int8_vanilla'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-135af81ab0ae>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"math\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"int8_vanilla\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"examples\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: 'int8_vanilla'"
          ]
        }
      ],
      "source": [
        "results[\"math\"][\"int8_vanilla\"][\"examples\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJ0vxxEMp7d_",
        "outputId": "646ec568-cbe2-4cbf-caeb-bc8501325139"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Location detected: Las Vegas, US (lat: 36.175, lon: -115.1372)\n",
            "Using estimated carbon intensity.\n",
            "Using estimate for US: 417 gCO2eq/kWh\n",
            "\n",
            "=== MATH SUMMARY ===\n",
            "    adaptive: E=406.00 J, Lat=12.241s, CO₂=940.5700g\n"
          ]
        }
      ],
      "source": [
        "# print summary for each task and mode\n",
        "ci = get_carbon_intensity()\n",
        "for task, modes in results.items():\n",
        "    print(f\"\\n=== {task.upper()} SUMMARY ===\")\n",
        "    for mode, data in modes.items():\n",
        "        summary = data.get(\"summary\", data)\n",
        "        e   = summary.get(\"avg_energy\",     summary.get(\"total_energy\", 0.0))\n",
        "        t   = summary.get(\"avg_time\",       summary.get(\"total_time\",   0.0))\n",
        "        # acc = summary.get(\"accuracy\",       None)\n",
        "        co2 = summary.get(\n",
        "            \"carbon_emissions\",\n",
        "            joules_to_co2(summary.get(\"total_energy\", e), ci)\n",
        "        )\n",
        "        line = f\"{mode:>12}: E={e:.2f} J, Lat={t:.3f}s\"\n",
        "        # if acc is not None:\n",
        "        #     line += f\", Acc={acc:.2f}%\"\n",
        "        line += f\", CO₂={co2:.4f}g\"\n",
        "        print(line)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y63kgYMsp9Xm"
      },
      "outputs": [],
      "source": [
        "# Plot overall energy comparison\n",
        "plot_energy_comparison(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jo3ttZ9BjeKc"
      },
      "outputs": [],
      "source": [
        "# Plot per-component breakdown for each task and mode\n",
        "for task, modes in results.items():\n",
        "    for mode in modes:\n",
        "        # skip modes without component stats\n",
        "        stat = results[task][mode]\n",
        "        comps = stat.get(\"summary\", stat).get(\"components\", None)\n",
        "        if comps:\n",
        "            plot_component_energy(results, task_type=task, quant_mode=mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kum6MkQjqAH9",
        "outputId": "cf2f5df3-f22c-412a-d358-a33272f929c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results saved to results.json\n"
          ]
        }
      ],
      "source": [
        "# save raw results to JSON\n",
        "with open(cfg[\"output_file\"], \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "print(f\"Results saved to {cfg['output_file']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FTTSkbxqBBI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.22"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
