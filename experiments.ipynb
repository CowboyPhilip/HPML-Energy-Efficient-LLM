{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQVHjJD4j4ig"
      },
      "outputs": [],
      "source": [
        "# run this cell if you are in colab with a single notebook opened, otherwise ignore this cell\n",
        "\n",
        "#!git clone https://github.com/CowboyPhilip/HPML-Energy-Efficient-LLM\n",
        "#%cd HPML-Energy-Efficient-LLM\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5G8E_rswj5Pf",
        "outputId": "cf33dfde-fdca-437e-9359-3f8c13a52be7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (80.2.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
            "Requirement already satisfied: zeus-ml in /usr/local/lib/python3.11/dist-packages (0.11.0.post1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: geocoder in /usr/local/lib/python3.11/dist-packages (1.38.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Collecting flash-attn==2.0.5\n",
            "  Using cached flash_attn-2.0.5.tar.gz (2.3 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting triton==2.0.0\n",
            "  Using cached triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.8.5.post1-cp38-abi3-manylinux1_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn==2.0.5) (0.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from flash-attn==2.0.5) (24.2)\n",
            "Collecting ninja (from flash-attn==2.0.5)\n",
            "  Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0) (3.31.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0) (3.18.0)\n",
            "Collecting lit (from triton==2.0.0)\n",
            "  Using cached lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch\n",
            "  Using cached torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n",
            "  Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n",
            "  Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting torch\n",
            "  Using cached torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "  Using cached torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "  Using cached torch-2.5.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "  Using cached torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting torch\n",
            "  Using cached torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "  Using cached torch-2.3.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "INFO: pip is still looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch\n",
            "  Using cached torch-2.3.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "  Using cached torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting torch\n",
            "  Using cached torch-2.2.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "  Using cached torch-2.2.0-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "  Using cached torch-2.1.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting nvidia-nccl-cu12==2.18.1 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting torch\n",
            "  Using cached torch-2.1.1-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "  Using cached torch-2.1.0-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "  Using cached torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch)\n",
            "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch)\n",
            "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
            "  Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch)\n",
            "  Using cached nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch)\n",
            "  Using cached nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch)\n",
            "  Using cached nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch)\n",
            "  Using cached nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (80.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.45.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from zeus-ml) (2.2.2)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.11/dist-packages (from zeus-ml) (12.570.86)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from zeus-ml) (2.11.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from zeus-ml) (13.9.4)\n",
            "Requirement already satisfied: tyro in /usr/local/lib/python3.11/dist-packages (from zeus-ml) (0.9.19)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from zeus-ml) (0.28.1)\n",
            "Requirement already satisfied: amdsmi in /usr/local/lib/python3.11/dist-packages (from zeus-ml) (6.4.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from zeus-ml) (2.9.0.post0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from geocoder) (8.1.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from geocoder) (1.0.0)\n",
            "Requirement already satisfied: ratelim in /usr/local/lib/python3.11/dist-packages (from geocoder) (0.1.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from geocoder) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from vllm) (5.5.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from vllm) (5.9.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from vllm) (0.2.0)\n",
            "Collecting blake3 (from vllm)\n",
            "  Using cached blake3-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from vllm) (9.0.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from vllm) (5.29.4)\n",
            "Collecting fastapi>=0.115.0 (from fastapi[standard]>=0.115.0->vllm)\n",
            "  Using cached fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: openai>=1.52.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.76.0)\n",
            "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from vllm) (11.2.1)\n",
            "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\n",
            "  Using cached prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting tiktoken>=0.6.0 (from vllm)\n",
            "  Using cached tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting lm-format-enforcer<0.11,>=0.10.11 (from vllm)\n",
            "  Using cached lm_format_enforcer-0.10.11-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting llguidance<0.8.0,>=0.7.9 (from vllm)\n",
            "  Using cached llguidance-0.7.19-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting outlines==0.1.11 (from vllm)\n",
            "  Using cached outlines-0.1.11-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting lark==1.2.2 (from vllm)\n",
            "  Using cached lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xgrammar==0.1.18 (from vllm)\n",
            "  Using cached xgrammar-0.1.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting partial-json-parser (from vllm)\n",
            "  Using cached partial_json_parser-0.2.1.1.post5-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting pyzmq>=25.0.0 (from vllm)\n",
            "  Using cached pyzmq-26.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.0 kB)\n",
            "Collecting msgspec (from vllm)\n",
            "  Using cached msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting gguf>=0.13.0 (from vllm)\n",
            "  Using cached gguf-0.16.2-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from vllm) (8.7.0)\n",
            "Collecting mistral_common>=1.5.4 (from mistral_common[opencv]>=1.5.4->vllm)\n",
            "  Using cached mistral_common-1.5.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.11.0.86)\n",
            "Collecting compressed-tensors==0.9.3 (from vllm)\n",
            "  Using cached compressed_tensors-0.9.3-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting depyf==0.18.0 (from vllm)\n",
            "  Using cached depyf-0.18.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from vllm) (3.1.1)\n",
            "Collecting watchfiles (from vllm)\n",
            "  Using cached watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting python-json-logger (from vllm)\n",
            "  Using cached python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting opentelemetry-sdk<1.27.0,>=1.26.0 (from vllm)\n",
            "  Using cached opentelemetry_sdk-1.26.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api<1.27.0,>=1.26.0 (from vllm)\n",
            "  Using cached opentelemetry_api-1.26.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp<1.27.0,>=1.26.0 (from vllm)\n",
            "  Using cached opentelemetry_exporter_otlp-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-semantic-conventions-ai<0.5.0,>=0.4.1 (from vllm)\n",
            "  Using cached opentelemetry_semantic_conventions_ai-0.4.5-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting numba==0.61.2 (from vllm)\n",
            "  Using cached numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting ray!=2.44.*,>=2.43.0 (from ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n",
            "  Using cached ray-2.45.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: torchaudio==2.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision==0.21.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.0+cu124)\n",
            "Collecting xformers==0.0.29.post2 (from vllm)\n",
            "  Using cached xformers-0.0.29.post2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.8.5-cp38-abi3-manylinux1_x86_64.whl.metadata (14 kB)\n",
            "  Using cached vllm-0.8.4-cp38-abi3-manylinux1_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.11/dist-packages (from vllm) (24.0.1)\n",
            "  Using cached vllm-0.8.3-cp38-abi3-manylinux1_x86_64.whl.metadata (27 kB)\n",
            "Collecting xgrammar==0.1.17 (from vllm)\n",
            "  Using cached xgrammar-0.1.17-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting gguf==0.10.0 (from vllm)\n",
            "  Using cached gguf-0.10.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting compressed-tensors==0.9.2 (from vllm)\n",
            "  Using cached compressed_tensors-0.9.2-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting numba==0.61 (from vllm)\n",
            "  Using cached numba-0.61.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.8.2-cp38-abi3-manylinux1_x86_64.whl.metadata (27 kB)\n",
            "Collecting numpy>=1.17 (from transformers)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting xgrammar==0.1.16 (from vllm)\n",
            "  Using cached xgrammar-0.1.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: numba==0.60.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.60.0)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.8.1-cp38-abi3-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "  Using cached vllm-0.8.0-cp38-abi3-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "  Using cached vllm-0.7.3-cp38-abi3-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting xgrammar==0.1.11 (from vllm)\n",
            "  Using cached xgrammar-0.1.11-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting compressed-tensors==0.9.1 (from vllm)\n",
            "  Using cached compressed_tensors-0.9.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting ray==2.40.0 (from ray[adag]==2.40.0->vllm)\n",
            "  Using cached ray-2.40.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (17 kB)\n",
            "Collecting torchaudio==2.5.1 (from vllm)\n",
            "  Using cached torchaudio-2.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting torchvision==0.20.1 (from vllm)\n",
            "  Using cached torchvision-0.20.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting xformers==0.0.28.post3 (from vllm)\n",
            "  Using cached xformers-0.0.28.post3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.7.2-cp38-abi3-manylinux1_x86_64.whl.metadata (12 kB)\n",
            "Collecting uvicorn[standard] (from vllm)\n",
            "  Using cached uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.7.1-cp38-abi3-manylinux1_x86_64.whl.metadata (12 kB)\n",
            "Collecting compressed-tensors==0.9.0 (from vllm)\n",
            "  Using cached compressed_tensors-0.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.7.0-cp38-abi3-manylinux1_x86_64.whl.metadata (12 kB)\n",
            "  Using cached vllm-0.6.6.post1-cp38-abi3-manylinux1_x86_64.whl.metadata (11 kB)\n",
            "Collecting compressed-tensors==0.8.1 (from vllm)\n",
            "  Using cached compressed_tensors-0.8.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.6.6-cp38-abi3-manylinux1_x86_64.whl.metadata (11 kB)\n",
            "  Using cached vllm-0.6.5-cp38-abi3-manylinux1_x86_64.whl.metadata (11 kB)\n",
            "  Using cached vllm-0.6.4.post1-cp38-abi3-manylinux1_x86_64.whl.metadata (10 kB)\n",
            "Collecting outlines<0.1,>=0.0.43 (from vllm)\n",
            "  Using cached outlines-0.0.46-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting compressed-tensors==0.8.0 (from vllm)\n",
            "  Using cached compressed_tensors-0.8.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.6.4-cp38-abi3-manylinux1_x86_64.whl.metadata (10 kB)\n",
            "Collecting lm-format-enforcer==0.10.6 (from vllm)\n",
            "  Using cached lm_format_enforcer-0.10.6-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.6.3.post1-cp38-abi3-manylinux1_x86_64.whl.metadata (10 kB)\n",
            "Collecting compressed-tensors==0.6.0 (from vllm)\n",
            "  Using cached compressed_tensors-0.6.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting torchvision==0.19 (from vllm)\n",
            "  Using cached torchvision-0.19.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "Collecting xformers==0.0.27.post2 (from vllm)\n",
            "  Using cached xformers-0.0.27.post2-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.6.3-cp38-abi3-manylinux1_x86_64.whl.metadata (10 kB)\n",
            "  Using cached vllm-0.6.2-cp38-abi3-manylinux1_x86_64.whl.metadata (2.4 kB)\n",
            "  Using cached vllm-0.6.1.post2-cp38-abi3-manylinux1_x86_64.whl.metadata (2.4 kB)\n",
            "Collecting gguf==0.9.1 (from vllm)\n",
            "  Using cached gguf-0.9.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting vllm-flash-attn==2.6.1 (from vllm)\n",
            "  Using cached vllm_flash_attn-2.6.1-cp311-cp311-manylinux1_x86_64.whl.metadata (476 bytes)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.6.1.post1-cp38-abi3-manylinux1_x86_64.whl.metadata (2.3 kB)\n",
            "  Using cached vllm-0.6.1-cp38-abi3-manylinux1_x86_64.whl.metadata (2.3 kB)\n",
            "  Using cached vllm-0.6.0-cp38-abi3-manylinux1_x86_64.whl.metadata (2.2 kB)\n",
            "  Using cached vllm-0.5.5-cp38-abi3-manylinux1_x86_64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from vllm) (0.11.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (from vllm) (0.13.1)\n",
            "  Using cached vllm-0.5.4-cp38-abi3-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting lm-format-enforcer==0.10.3 (from vllm)\n",
            "  Using cached lm_format_enforcer-0.10.3-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.5.3.post1-cp38-abi3-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting torchvision==0.18.1 (from vllm)\n",
            "  Using cached torchvision-0.18.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting xformers==0.0.27 (from vllm)\n",
            "  Using cached xformers-0.0.27-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting vllm-flash-attn==2.5.9.post1 (from vllm)\n",
            "  Using cached vllm_flash_attn-2.5.9.post1-cp311-cp311-manylinux1_x86_64.whl.metadata (482 bytes)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.5.3-cp38-abi3-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "  Using cached vllm-0.5.2-cp38-abi3-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "  Using cached vllm-0.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (8.2 kB)\n",
            "Collecting lm-format-enforcer==0.10.1 (from vllm)\n",
            "  Using cached lm_format_enforcer-0.10.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting outlines>=0.0.43 (from vllm)\n",
            "  Using cached outlines-0.2.3-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting torchvision==0.18.0 (from vllm)\n",
            "  Using cached torchvision-0.18.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting xformers==0.0.26.post1 (from vllm)\n",
            "  Using cached xformers-0.0.26.post1-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting vllm-flash-attn==2.5.9 (from vllm)\n",
            "  Using cached vllm_flash_attn-2.5.9-cp311-cp311-manylinux1_x86_64.whl.metadata (476 bytes)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.5.0.post1-cp311-cp311-manylinux1_x86_64.whl.metadata (8.1 kB)\n",
            "  Using cached vllm-0.5.0-cp311-cp311-manylinux1_x86_64.whl.metadata (8.3 kB)\n",
            "  Using cached vllm-0.4.3-cp311-cp311-manylinux1_x86_64.whl.metadata (7.8 kB)\n",
            "Collecting outlines==0.0.34 (from vllm)\n",
            "  Using cached outlines-0.0.34-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting vllm-flash-attn==2.5.8.post2 (from vllm)\n",
            "  Using cached vllm_flash_attn-2.5.8.post2-cp311-cp311-manylinux1_x86_64.whl.metadata (482 bytes)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.4.2-cp311-cp311-manylinux1_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting tiktoken==0.6.0 (from vllm)\n",
            "  Using cached tiktoken-0.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting lm-format-enforcer==0.9.8 (from vllm)\n",
            "  Using cached lm_format_enforcer-0.9.8-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting vllm-nccl-cu12<2.19,>=2.18 (from vllm)\n",
            "  Using cached vllm_nccl_cu12-2.18.1.0.4.0.tar.gz (6.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.4.1-cp311-cp311-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
            "Collecting xformers==0.0.25 (from vllm)\n",
            "  Using cached xformers-0.0.25-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.4.0.post1-cp311-cp311-manylinux1_x86_64.whl.metadata (8.6 kB)\n",
            "Collecting xformers==0.0.23.post1 (from vllm)\n",
            "  Using cached xformers-0.0.23.post1-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting pynvml==11.5.0 (from vllm)\n",
            "  Using cached pynvml-11.5.0-py3-none-any.whl.metadata (7.8 kB)\n",
            "INFO: pip is looking at multiple versions of vllm to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (8.6 kB)\n",
            "  Using cached vllm-0.3.3-cp311-cp311-manylinux1_x86_64.whl.metadata (7.8 kB)\n",
            "  Using cached vllm-0.3.2-cp311-cp311-manylinux1_x86_64.whl.metadata (7.5 kB)\n",
            "Collecting aioprometheus[starlette] (from vllm)\n",
            "  Using cached aioprometheus-23.12.0-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.3.1-cp311-cp311-manylinux1_x86_64.whl.metadata (7.4 kB)\n",
            "  Using cached vllm-0.3.0-cp311-cp311-manylinux1_x86_64.whl.metadata (7.4 kB)\n",
            "INFO: pip is still looking at multiple versions of vllm to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached vllm-0.2.7-cp311-cp311-manylinux1_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting pydantic (from zeus-ml)\n",
            "  Using cached pydantic-1.10.13-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (149 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.2.6-cp311-cp311-manylinux1_x86_64.whl.metadata (6.7 kB)\n",
            "  Using cached vllm-0.2.5-cp311-cp311-manylinux1_x86_64.whl.metadata (6.5 kB)\n",
            "Collecting xformers>=0.0.23 (from vllm)\n",
            "  Using cached xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.2.4-cp311-cp311-manylinux1_x86_64.whl.metadata (6.8 kB)\n",
            "  Using cached vllm-0.2.3-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "  Using cached vllm-0.2.2-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "  Using cached vllm-0.2.1.post1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.2 kB)\n",
            "Collecting xformers==0.0.22 (from vllm)\n",
            "  Using cached xformers-0.0.22-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting pydantic (from zeus-ml)\n",
            "  Using cached pydantic-1.10.22-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (154 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.1.0)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm)\n",
            "  Using cached starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (4.9.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->zeus-ml) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->zeus-ml) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (0.24.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->zeus-ml) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->zeus-ml) (2025.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ratelim->geocoder) (4.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->zeus-ml) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->zeus-ml) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->zeus-ml) (0.1.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->zeus-ml) (0.16)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro->zeus-ml) (1.7.2)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->zeus-ml) (4.4.2)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]->vllm)\n",
            "  Using cached httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]->vllm)\n",
            "  Using cached python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]->vllm)\n",
            "  Using cached uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm) (15.0.1)\n",
            "Using cached triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "Using cached torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\n",
            "Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "Using cached nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "Downloading vllm-0.2.1.post1-cp311-cp311-manylinux1_x86_64.whl (28.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.7/28.7 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.22-cp311-cp311-manylinux2014_x86_64.whl (211.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.6/211.6 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-1.10.22-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ray-2.45.0-cp311-cp311-manylinux2014_x86_64.whl (68.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.4/68.4 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "Downloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n",
            "Building wheels for collected packages: flash-attn\n",
            "\u001b[33m  DEPRECATION: Building 'flash-attn' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'flash-attn'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for flash-attn\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for flash-attn\n",
            "Failed to build flash-attn\n",
            "\u001b[31mERROR: Failed to build installable wheels for some pyproject.toml based projects (flash-attn)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# 1. Install dependencies\n",
        "!pip install --upgrade pip setuptools\n",
        "!pip install \\\n",
        "    transformers \\\n",
        "    bitsandbytes \\\n",
        "    zeus-ml \\\n",
        "    torch \\\n",
        "    datasets \\\n",
        "    evaluate \\\n",
        "    scikit-learn \\\n",
        "    geocoder \\\n",
        "    requests \\\n",
        "    flash-attn==2.0.5 \\\n",
        "    triton==2.0.0 \\\n",
        "    vllm \\\n",
        "    numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "YVOgToaUjT50"
      },
      "outputs": [],
      "source": [
        "# global configuration for experiments\n",
        "cfg = {\n",
        "    \"task\":           \"math\",\n",
        "    \"model\":          \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
        "    # test default (vanilla) kernel at fp16/int8/int4, plus adaptive switching\n",
        "    \"modes\":          [\n",
        "        \"fp16_vanilla\",    # FP16 + vanilla Transformer\n",
        "        \"int8_vanilla\",    # INT8 + vanilla\n",
        "        \"int4_vanilla\",    # INT4 + vanilla& low_mode\n",
        "    ],\n",
        "    # for adaptive mode: which two modes to switch between\n",
        "    \"high_mode\":      \"fp16_vanilla\",\n",
        "    \"low_mode\":       \"int8_vanilla\",\n",
        "\n",
        "    \"dataset_name\":   \"deepmind/math_dataset\",\n",
        "    \"dataset_config\": \"algebra__linear_1d\",\n",
        "    \"split\":          \"test\",\n",
        "    \"num_examples\":   20,\n",
        "    \"subjects\":       [\"physics\",\"chemistry\"],\n",
        "    \"quick\":          True,\n",
        "    \"max_samples\":    20,\n",
        "    \"glue_tasks\":     [\"sst2\",\"cola\"],\n",
        "    \"batch_size\":     1,\n",
        "    \"verbose\":        True,\n",
        "    \"output_file\":    \"results.json\"\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "r5e1v5JwvDfu"
      },
      "outputs": [],
      "source": [
        "cfg = {\n",
        "    \"task\":        \"math\",                              # only MATH\n",
        "    \"model\":       \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
        "    \n",
        "\n",
        "    \"modes\":       [\"adaptive\"],                        # only adaptive\n",
        "    \"high_mode\":   \"fp16_vanilla\",                      # high precision = FP16 + vanilla\n",
        "    \"low_mode\":    \"int8_vanilla\",                      # low precision = INT8 + vanilla\n",
        "\n",
        "    # MATH dataset\n",
        "    \"dataset_name\":   \"deepmind/math_dataset\",\n",
        "    \"dataset_config\": \"algebra__linear_1d\",\n",
        "    \"split\":      \"test\",\n",
        "    \"num_examples\":   20,\n",
        "\n",
        "    \"verbose\":     True,\n",
        "    \"output_file\": \"adaptive_math_results.json\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# exp for mbpp\n",
        "cfg = {\n",
        "    \"task\":           \"mbpp\",\n",
        "    # \"model\":          \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
        "    \"model\":       \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "    # test default (vanilla) kernel at fp16/int8/int4, plus adaptive switching\n",
        "    \"modes\":          [\n",
        "        \"fp16_flash-v2\",    # FP16 + flash attn v2 Transformer\n",
        "        # \"int8_vanilla\",    # INT8 + vanilla\n",
        "        # \"int4_vanilla\",    # INT4 + vanilla& low_mode\n",
        "    ],\n",
        "    # for adaptive mode: which two modes to switch between\n",
        "    \"high_mode\":      \"fp16_vanilla\",\n",
        "    \"low_mode\":       \"int8_vanilla\",\n",
        "\n",
        "    \"dataset_name\":   \"deepmind/math_dataset\",\n",
        "    \"dataset_config\": \"algebra__linear_1d\",\n",
        "    \"split\":          \"test\",\n",
        "    \"num_examples\":   10,\n",
        "    \"subjects\":       [\"physics\",\"chemistry\"],\n",
        "    \"quick\":          True,\n",
        "    \"max_samples\":    500,\n",
        "    \"glue_tasks\":     [\"sst2\",\"cola\"],\n",
        "    \"batch_size\":     1,\n",
        "    \"verbose\":        True,\n",
        "    \"output_file\":    \"results.json\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yA9v4noz0XtJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/vLLM/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/opt/rocm/lib/libamd_smi.so: cannot open shared object file: No such file or directory\n",
            "Unable to find libamd_smi.so library try installing amd-smi-lib from your package manager\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset  # ensure load_dataset is defined\n",
        "\n",
        "# benchmark functions\n",
        "from utils.test_generation import quick_test_generation, test_generation_MATH, test_generation_MBPP\n",
        "from utils.test_mmlu      import quick_test_mmlu, test_quantized_models_on_mmlu\n",
        "from utils.test_glue      import test_quantized_models_on_glue\n",
        "\n",
        "# energy & tracking\n",
        "from utils.energy_utils   import EnergyTracker, get_carbon_intensity, joules_to_co2\n",
        "from utils.memory_utils   import clean_memory\n",
        "\n",
        "# adaptive quant wrapper\n",
        "from utils.adaptive_quant      import AdaptiveQuantGenerator\n",
        "\n",
        "# plotting\n",
        "from utils.plot_utils    import plot_energy_comparison, plot_component_energy\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1oM_9fRa1Qq5"
      },
      "outputs": [],
      "source": [
        "# Monkey-patch EnergyTracker to support `with tracker:` and save_results\n",
        "def _et_enter(self):\n",
        "    if getattr(self, 'zeus', None):\n",
        "        try:\n",
        "            self.zeus.begin_window('inference')\n",
        "            self.active_windows.add('inference')\n",
        "        except:\n",
        "            pass\n",
        "    self._enter_ts = time.time()\n",
        "    return self\n",
        "\n",
        "def _et_exit(self, exc_type, exc_val, exc_tb):\n",
        "    end_ts = time.time()\n",
        "    inf_e = 0\n",
        "    if getattr(self, 'zeus', None) and 'inference' in self.active_windows:\n",
        "        try:\n",
        "            m = self.zeus.end_window('inference')\n",
        "            inf_e = m.total_energy\n",
        "            self.active_windows.remove('inference')\n",
        "        except:\n",
        "            pass\n",
        "    elapsed = end_ts - getattr(self, '_enter_ts', end_ts)\n",
        "    comp = {k: np.sum(v) for k, v in self.comp_energy.items()}\n",
        "    self.stats = {\n",
        "        'total_energy': inf_e,\n",
        "        'time': elapsed,\n",
        "        'components': comp,\n",
        "        'num_tokens': None\n",
        "    }\n",
        "    return False\n",
        "\n",
        "def _save_results(self, extra_metrics):\n",
        "    if not hasattr(self, 'stats'):\n",
        "        self.stats = {}\n",
        "    self.stats.update(extra_metrics)\n",
        "\n",
        "EnergyTracker.__enter__    = _et_enter\n",
        "EnergyTracker.__exit__     = _et_exit\n",
        "EnergyTracker.save_results = _save_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BgCZi3UfjVih"
      },
      "outputs": [],
      "source": [
        "def run_task(cfg):\n",
        "    \"\"\"Dispatch benchmarks based on cfg['task'].\"\"\"\n",
        "    task = cfg[\"task\"]\n",
        "    modes = list(cfg[\"modes\"])\n",
        "    results = {}\n",
        "\n",
        "    # skip adaptive for pure classification tasks\n",
        "    if task in (\"glue\", \"mmlu\") and \"adaptive\" in modes:\n",
        "        print(\"⚠️  Skipping adaptive for classification tasks\")\n",
        "        modes.remove(\"adaptive\")\n",
        "\n",
        "    # text generation benchmark\n",
        "    if task == \"generation\":\n",
        "        results[\"generation\"] = {}\n",
        "        # adaptive mode\n",
        "        if \"adaptive\" in modes:\n",
        "            print(\"\\n=== ADAPTIVE generation ===\")\n",
        "            agent = AdaptiveQuantGenerator(\n",
        "                cfg[\"model\"],\n",
        "                high_mode=cfg[\"high_mode\"],\n",
        "                low_mode=cfg[\"low_mode\"]\n",
        "            )\n",
        "            _ = agent.generate(cfg[\"prompt\"], max_new_tokens=cfg[\"tokens\"])\n",
        "            results[\"generation\"][\"adaptive\"] = {\"note\": \"see adaptive_quant logs\"}\n",
        "            modes.remove(\"adaptive\")\n",
        "        # other quant/kernel modes\n",
        "        for mode in modes:\n",
        "            print(f\"\\n=== {mode.upper()} generation ===\")\n",
        "            stats = quick_test_generation(\n",
        "                model_name=cfg[\"model\"],\n",
        "                quant_mode=mode,\n",
        "                prompt=cfg[\"prompt\"],\n",
        "                max_new_tokens=cfg[\"tokens\"]\n",
        "            )\n",
        "            results[\"generation\"][mode] = stats\n",
        "\n",
        "    # MATH dataset benchmark (generation-style)\n",
        "    elif task == \"math\":\n",
        "        results[\"math\"] = {}\n",
        "        # adaptive on MATH\n",
        "        if \"adaptive\" in modes:\n",
        "            print(\"\\n=== ADAPTIVE on MATH ===\")\n",
        "            ds = load_dataset(\n",
        "                cfg[\"dataset_name\"],\n",
        "                cfg[\"dataset_config\"],\n",
        "                split=cfg[\"split\"]\n",
        "            ).select(range(cfg[\"num_examples\"]))\n",
        "            adapter = AdaptiveQuantGenerator(\n",
        "                cfg[\"model\"],\n",
        "                high_mode=cfg[\"high_mode\"],\n",
        "                low_mode=cfg[\"low_mode\"]\n",
        "            )\n",
        "            examples, correct, total_tokens = [], 0, 0\n",
        "            for ex in tqdm(ds, desc=\"Adaptive MATH\"):\n",
        "                q, ans = ex[\"question\"], ex[\"answer\"].strip()\n",
        "                tracker = EnergyTracker(\"adaptive_quant\")\n",
        "                with tracker:\n",
        "                    out = adapter.generate(q, max_new_tokens=32)\n",
        "                pred = out.strip()\n",
        "                correct += int(pred == ans)\n",
        "                total_tokens += tracker.stats.get(\"tokens_generated\", 1)\n",
        "                examples.append({\n",
        "                    \"question\": q,\n",
        "                    \"prediction\": pred,\n",
        "                    \"is_correct\": pred == ans,\n",
        "                    \"stats\": tracker.stats\n",
        "                })\n",
        "                clean_memory()\n",
        "            n = len(examples)\n",
        "            total_e = sum(e[\"stats\"][\"total_energy\"] for e in examples)\n",
        "            total_t = sum(e[\"stats\"][\"time\"]         for e in examples)\n",
        "            results[\"math\"][\"adaptive\"] = {\n",
        "                \"examples\": examples,\n",
        "                \"summary\": {\n",
        "                    \"accuracy\":         100 * correct / n,\n",
        "                    \"avg_energy\":       total_e / n,\n",
        "                    \"avg_time\":         total_t / n,\n",
        "                    \"energy_per_token\": total_e / total_tokens,\n",
        "                    \"carbon_emissions\": joules_to_co2(total_e, get_carbon_intensity())\n",
        "                }\n",
        "            }\n",
        "            plot_component_energy(results, task_type=\"math\", quant_mode=\"adaptive\")\n",
        "            modes.remove(\"adaptive\")\n",
        "\n",
        "        # standard quant modes on MATH\n",
        "        if modes:\n",
        "            print(f\"\\n=== standard modes on MATH: {modes} ===\")\n",
        "            std = test_generation_MATH(\n",
        "                model_name=cfg[\"model\"],\n",
        "                quantization_modes=modes,\n",
        "                dataset_name=cfg[\"dataset_name\"],\n",
        "                dataset_config=cfg[\"dataset_config\"],\n",
        "                split=cfg[\"split\"],\n",
        "                num_examples=cfg[\"num_examples\"],\n",
        "                verbose=cfg[\"verbose\"]\n",
        "            )\n",
        "            results[\"math\"].update(std)\n",
        "\n",
        "    # MBPP dataset benchmark\n",
        "    elif task == \"mbpp\":\n",
        "        print(\"\\n=== MBPP task ===\")\n",
        "        results[\"mbpp\"] = test_generation_MBPP(\n",
        "            model_name=cfg[\"model\"],\n",
        "            quantization_modes=modes,\n",
        "            num_examples=cfg[\"num_examples\"],\n",
        "            verbose=cfg[\"verbose\"]\n",
        "        )\n",
        "\n",
        "    # MMLU multiple-choice benchmark\n",
        "    elif task == \"mmlu\":\n",
        "        print(\"\\n=== MMLU task ===\")\n",
        "        if cfg.get(\"quick\", False):\n",
        "            stats = quick_test_mmlu(\n",
        "                model_name=cfg[\"model\"],\n",
        "                quant_mode=modes[0],\n",
        "                subjects=cfg[\"subjects\"],\n",
        "                max_samples=cfg[\"max_samples\"]\n",
        "            )\n",
        "        else:\n",
        "            stats = test_quantized_models_on_mmlu(\n",
        "                model_name=cfg[\"model\"],\n",
        "                quantization_modes=modes,\n",
        "                subjects=cfg[\"subjects\"]\n",
        "            )\n",
        "        results[\"mmlu\"] = stats\n",
        "\n",
        "    # GLUE classification benchmark\n",
        "    else:\n",
        "        print(\"\\n=== GLUE task ===\")\n",
        "        results[\"glue\"] = test_quantized_models_on_glue(\n",
        "            model_name=cfg[\"model\"],\n",
        "            tasks=cfg[\"glue_tasks\"],\n",
        "            quantization_modes=modes,\n",
        "            batch_size=cfg[\"batch_size\"]\n",
        "        )\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUBRs4YajWOq",
        "outputId": "91d3c643-f8b1-4899-f1cb-81d28956550d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== MBPP task ===\n",
            "Location detected: São Paulo, BR (lat: -23.5475, lon: -46.6361)\n",
            "Using estimated carbon intensity.\n",
            "Using estimate for BR: 110 gCO2eq/kWh\n",
            "Carbon intensity: 110 gCO2eq/kWh\n",
            "\n",
            "===== Testing FP16_FLASH-V2 Mode on MBPP =====\n",
            "Loading FP16 model …\n",
            "GPU Memory: Allocated: 0.00 GB | Reserved: 0.00 GB | Max: 0.00 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model ready → quantisation: FP16, kernel: flash-v2\n",
            "GPU Memory: Allocated: 3.55 GB | Reserved: 7.26 GB | Max: 7.12 GB\n",
            "[2025-05-06 19:49:41,004] [zeus.device.gpu.nvidia](nvidia.py:47) pynvml is available and initialized.\n",
            "[2025-05-06 19:49:41,012] [zeus.device.cpu.rapl](rapl.py:137) RAPL is not supported on this CPU.\n",
            "[2025-05-06 19:49:41,013] [zeus.monitor.energy](energy.py:209) Monitoring GPU indices [0].\n",
            "[2025-05-06 19:49:41,014] [zeus.monitor.energy](energy.py:210) Monitoring CPU indices []\n",
            "Successfully initialized ZeusMonitor\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing FP16_FLASH-V2:   0%|          | 0/500 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-05-06 19:49:41,132] [zeus.utils.framework](framework.py:25) PyTorch with CUDA support is available.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing FP16_FLASH-V2:   2%|▏         | 10/500 [00:08<06:35,  1.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Summary =====\n",
            "Mode | Avg Energy per Infer(J) | Avg Time per Infer (s) | Energy/Token (J) | Accuracy (%) | CO2 (gCO2eq)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "FP16_FLASH-V2 | 30.7857 | 0.788 | 1.293517 | 0.00 | 9.406744\n",
            "\n",
            "Component Energy Breakdown for FP16_FLASH-V2\n",
            "  ffn: 95.9747 J (47.8%)\n",
            "  attention: 83.3830 J (41.5%)\n",
            "  output_layer: 13.6563 J (6.8%)\n",
            "  embeddings: 6.6113 J (3.3%)\n",
            "  layernorm: 1.1731 J (0.6%)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'mbpp': {'fp16_flash-v2': {'examples': [{'prompt': 'output only the code, no explanation: Write a python function to remove first and last occurrence of a given character from the string.',\n",
              "     'ground_truth_code': 'def remove_Occ(s,ch): \\r\\n    for i in range(len(s)): \\r\\n        if (s[i] == ch): \\r\\n            s = s[0 : i] + s[i + 1:] \\r\\n            break\\r\\n    for i in range(len(s) - 1,-1,-1):  \\r\\n        if (s[i] == ch): \\r\\n            s = s[0 : i] + s[i + 1:] \\r\\n            break\\r\\n    return s ',\n",
              "     'generated_code': ')\\n\\n the the number.\\n\\n no explanation or # a Python script that compute duplicate occurrence last character of a character number in a string, If',\n",
              "     'test_cases': ['assert remove_Occ(\"hello\",\"l\") == \"heo\"',\n",
              "      'assert remove_Occ(\"abcda\",\"a\") == \"bcd\"',\n",
              "      'assert remove_Occ(\"PHP\",\"P\") == \"H\"'],\n",
              "     'is_correct': False,\n",
              "     'stats': {'total_energy': 60.978000000002794,\n",
              "      'tokenization_energy': 2.719000000040978,\n",
              "      'inference_energy': 58.258999999961816,\n",
              "      'energy_per_token': 2.2584444444445477,\n",
              "      'time': 2.1747236251831055,\n",
              "      'components': {'embeddings': np.float64(5.429000000003725),\n",
              "       'attention': np.float64(12.25662049837038),\n",
              "       'ffn': np.float64(14.297279810952023),\n",
              "       'layernorm': np.float64(0.11149679088592529),\n",
              "       'output_layer': np.float64(0.6261237981319427)},\n",
              "      'num_tokens': 27}},\n",
              "    {'prompt': 'output only the code, no explanation: Write a function to sort a given matrix in ascending order according to the sum of its rows.',\n",
              "     'ground_truth_code': 'def sort_matrix(M):\\r\\n    result = sorted(M, key=sum)\\r\\n    return result',\n",
              "     'generated_code': ')\\n\\n the the number.\\n\\n no explanation or # a Python that compute a list array into ascending order, to the elements of each elements.\\n If',\n",
              "     'test_cases': ['assert sort_matrix([[1, 2, 3], [2, 4, 5], [1, 1, 1]])==[[1, 1, 1], [1, 2, 3], [2, 4, 5]]',\n",
              "      'assert sort_matrix([[1, 2, 3], [-2, 4, -5], [1, -1, 1]])==[[-2, 4, -5], [1, -1, 1], [1, 2, 3]]',\n",
              "      'assert sort_matrix([[5,8,9],[6,4,3],[2,1,4]])==[[2, 1, 4], [6, 4, 3], [5, 8, 9]]'],\n",
              "     'is_correct': False,\n",
              "     'stats': {'total_energy': 25.376579460222274,\n",
              "      'tokenization_energy': 0.11757946014404297,\n",
              "      'inference_energy': 25.25900000007823,\n",
              "      'energy_per_token': 0.9063064092936527,\n",
              "      'time': 0.6727879047393799,\n",
              "      'components': {'embeddings': np.float64(0.0889694094657898),\n",
              "       'attention': np.float64(4.41078684782982),\n",
              "       'ffn': np.float64(13.462225658763202),\n",
              "       'layernorm': np.float64(0.11719379377365112),\n",
              "       'output_layer': np.float64(0.6129258463382721)},\n",
              "      'num_tokens': 28}},\n",
              "    {'prompt': 'output only the code, no explanation: Write a function to count the most common words in a dictionary.',\n",
              "     'ground_truth_code': 'from collections import Counter\\r\\ndef count_common(words):\\r\\n  word_counts = Counter(words)\\r\\n  top_four = word_counts.most_common(4)\\r\\n  return (top_four)\\r\\n',\n",
              "     'generated_code': ')\\n\\n the the number.\\n\\n no explanation or # a Python that compute the number frequent words in a text, The',\n",
              "     'test_cases': ['assert count_common([\\'red\\',\\'green\\',\\'black\\',\\'pink\\',\\'black\\',\\'white\\',\\'black\\',\\'eyes\\',\\'white\\',\\'black\\',\\'orange\\',\\'pink\\',\\'pink\\',\\'red\\',\\'red\\',\\'white\\',\\'orange\\',\\'white\\',\"black\",\\'pink\\',\\'green\\',\\'green\\',\\'pink\\',\\'green\\',\\'pink\\',\\'white\\',\\'orange\\',\"orange\",\\'red\\']) == [(\\'pink\\', 6), (\\'black\\', 5), (\\'white\\', 5), (\\'red\\', 4)]',\n",
              "      \"assert count_common(['one', 'two', 'three', 'four', 'five', 'one', 'two', 'one', 'three', 'one']) == [('one', 4), ('two', 2), ('three', 2), ('four', 1)]\",\n",
              "      \"assert count_common(['Facebook', 'Apple', 'Amazon', 'Netflix', 'Google', 'Apple', 'Netflix', 'Amazon']) == [('Apple', 2), ('Amazon', 2), ('Netflix', 2), ('Facebook', 1)]\"],\n",
              "     'is_correct': False,\n",
              "     'stats': {'total_energy': 30.226999999955297,\n",
              "      'tokenization_energy': 4.87300000002142,\n",
              "      'inference_energy': 25.353999999933876,\n",
              "      'energy_per_token': 1.3739545454525135,\n",
              "      'time': 0.6690995693206787,\n",
              "      'components': {'embeddings': np.float64(0.1745864248275757),\n",
              "       'attention': np.float64(8.355389833221212),\n",
              "       'ffn': np.float64(5.638681482791901),\n",
              "       'layernorm': np.float64(0.11537155532836915),\n",
              "       'output_layer': np.float64(0.5992143669128418)},\n",
              "      'num_tokens': 22}},\n",
              "    {'prompt': 'output only the code, no explanation: Write a python function to find the volume of a triangular prism.',\n",
              "     'ground_truth_code': 'def find_Volume(l,b,h) : \\r\\n    return ((l * b * h) / 2) ',\n",
              "     'generated_code': ')\\n\\n the the number.\\n\\n no explanation or # a Python script that compute the maximum of a rectangular prism.\\n The',\n",
              "     'test_cases': ['assert find_Volume(10,8,6) == 240',\n",
              "      'assert find_Volume(3,2,2) == 6',\n",
              "      'assert find_Volume(1,2,1) == 1'],\n",
              "     'is_correct': False,\n",
              "     'stats': {'total_energy': 26.617441054331138,\n",
              "      'tokenization_energy': 0.18144105434417726,\n",
              "      'inference_energy': 26.43599999998696,\n",
              "      'energy_per_token': 1.209883684287779,\n",
              "      'time': 0.6203079223632812,\n",
              "      'components': {'embeddings': np.float64(0.11310289764404298),\n",
              "       'attention': np.float64(9.208363048064525),\n",
              "       'ffn': np.float64(5.654500673055649),\n",
              "       'layernorm': np.float64(0.11909227037429809),\n",
              "       'output_layer': np.float64(0.6180503840446472)},\n",
              "      'num_tokens': 22}},\n",
              "    {'prompt': 'output only the code, no explanation: Write a function to split a string at lowercase letters.',\n",
              "     'ground_truth_code': \"import re\\r\\ndef split_lowerstring(text):\\r\\n return (re.findall('[a-z][^a-z]*', text))\",\n",
              "     'generated_code': ')\\n\\n the the number.\\n\\n no explanation or # a Python that compute a number into the letters, So',\n",
              "     'test_cases': ['assert split_lowerstring(\"AbCd\")==[\\'bC\\',\\'d\\']',\n",
              "      'assert split_lowerstring(\"Python\")==[\\'y\\', \\'t\\', \\'h\\', \\'o\\', \\'n\\']',\n",
              "      'assert split_lowerstring(\"Programming\")==[\\'r\\', \\'o\\', \\'g\\', \\'r\\', \\'a\\', \\'m\\', \\'m\\', \\'i\\', \\'n\\', \\'g\\']'],\n",
              "     'is_correct': False,\n",
              "     'stats': {'total_energy': 26.611324936474674,\n",
              "      'tokenization_energy': 0.1953249363899231,\n",
              "      'inference_energy': 26.41600000008475,\n",
              "      'energy_per_token': 1.3305662468237336,\n",
              "      'time': 0.6204802989959717,\n",
              "      'components': {'embeddings': np.float64(0.11528489112854004),\n",
              "       'attention': np.float64(9.183053193135189),\n",
              "       'ffn': np.float64(5.654336509466172),\n",
              "       'layernorm': np.float64(0.12019832348823548),\n",
              "       'output_layer': np.float64(0.6232188272476197)},\n",
              "      'num_tokens': 20}},\n",
              "    {'prompt': 'output only the code, no explanation: Write a function to find sequences of lowercase letters joined with an underscore.',\n",
              "     'ground_truth_code': \"import re\\r\\ndef text_lowercase_underscore(text):\\r\\n        patterns = '^[a-z]+_[a-z]+$'\\r\\n        if re.search(patterns,  text):\\r\\n                return 'Found a match!'\\r\\n        else:\\r\\n                return('Not matched!')\",\n",
              "     'generated_code': ')\\n\\n the the number.\\n\\n no explanation or # a Python that compute the of consecutive letters in by exactly empty, The',\n",
              "     'test_cases': ['assert text_lowercase_underscore(\"aab_cbbbc\")==(\\'Found a match!\\')',\n",
              "      'assert text_lowercase_underscore(\"aab_Abbbc\")==(\\'Not matched!\\')',\n",
              "      'assert text_lowercase_underscore(\"Aaab_abbbc\")==(\\'Not matched!\\')'],\n",
              "     'is_correct': False,\n",
              "     'stats': {'total_energy': 30.826999999932013,\n",
              "      'tokenization_energy': 4.779999999911524,\n",
              "      'inference_energy': 26.04700000002049,\n",
              "      'energy_per_token': 1.3403043478231311,\n",
              "      'time': 0.6292083263397217,\n",
              "      'components': {'embeddings': np.float64(0.11379809188842774),\n",
              "       'attention': np.float64(4.859356997489929),\n",
              "       'ffn': np.float64(10.269789235697127),\n",
              "       'layernorm': np.float64(0.12595327138900758),\n",
              "       'output_layer': np.float64(4.3269999999320135)},\n",
              "      'num_tokens': 23}},\n",
              "    {'prompt': 'output only the code, no explanation: Write a function to find the perimeter of a square.',\n",
              "     'ground_truth_code': 'def square_perimeter(a):\\r\\n  perimeter=4*a\\r\\n  return perimeter',\n",
              "     'generated_code': ')\\n\\n the the number.\\n\\n no explanation or # a Python that compute the maximum of a rectangle given The',\n",
              "     'test_cases': ['assert square_perimeter(10)==40',\n",
              "      'assert square_perimeter(5)==20',\n",
              "      'assert square_perimeter(4)==16'],\n",
              "     'is_correct': False,\n",
              "     'stats': {'total_energy': 26.68027259277273,\n",
              "      'tokenization_energy': 0.14127259278297424,\n",
              "      'inference_energy': 26.538999999989755,\n",
              "      'energy_per_token': 1.3340136296386365,\n",
              "      'time': 0.6254081726074219,\n",
              "      'components': {'embeddings': np.float64(0.2274829092025757),\n",
              "       'attention': np.float64(4.306249331712723),\n",
              "       'ffn': np.float64(23.649081901310943),\n",
              "       'layernorm': np.float64(0.10839490890502929),\n",
              "       'output_layer': np.float64(0.5655738317966461)},\n",
              "      'num_tokens': 20}},\n",
              "    {'prompt': 'output only the code, no explanation: Write a function to remove characters from the first string which are present in the second string.',\n",
              "     'ground_truth_code': \"NO_OF_CHARS = 256\\r\\ndef str_to_list(string): \\r\\n\\ttemp = [] \\r\\n\\tfor x in string: \\r\\n\\t\\ttemp.append(x) \\r\\n\\treturn temp \\r\\ndef lst_to_string(List): \\r\\n\\treturn ''.join(List) \\r\\ndef get_char_count_array(string): \\r\\n\\tcount = [0] * NO_OF_CHARS \\r\\n\\tfor i in string: \\r\\n\\t\\tcount[ord(i)] += 1\\r\\n\\treturn count \\r\\ndef remove_dirty_chars(string, second_string): \\r\\n\\tcount = get_char_count_array(second_string) \\r\\n\\tip_ind = 0\\r\\n\\tres_ind = 0\\r\\n\\ttemp = '' \\r\\n\\tstr_list = str_to_list(string) \\r\\n\\twhile ip_ind != len(str_list): \\r\\n\\t\\ttemp = str_list[ip_ind] \\r\\n\\t\\tif count[ord(temp)] == 0: \\r\\n\\t\\t\\tstr_list[res_ind] = str_list[ip_ind] \\r\\n\\t\\t\\tres_ind += 1\\r\\n\\t\\tip_ind+=1\\r\\n\\treturn lst_to_string(str_list[0:res_ind]) \",\n",
              "     'generated_code': ')\\n\\n the the number.\\n\\n no explanation or # a Python that compute duplicate from a string and that are also in the second string.\\n Make',\n",
              "     'test_cases': ['assert remove_dirty_chars(\"probasscurve\", \"pros\") == \\'bacuve\\'',\n",
              "      'assert remove_dirty_chars(\"digitalindia\", \"talent\") == \\'digiidi\\'',\n",
              "      'assert remove_dirty_chars(\"exoticmiles\", \"toxic\") == \\'emles\\' '],\n",
              "     'is_correct': False,\n",
              "     'stats': {'total_energy': 26.874321009418928,\n",
              "      'tokenization_energy': 0.1263210093975067,\n",
              "      'inference_energy': 26.74800000002142,\n",
              "      'energy_per_token': 0.9953452225710714,\n",
              "      'time': 0.627277135848999,\n",
              "      'components': {'embeddings': np.float64(0.10443732500076293),\n",
              "       'attention': np.float64(8.449179009854793),\n",
              "       'ffn': np.float64(5.772169494152069),\n",
              "       'layernorm': np.float64(0.11969486379623413),\n",
              "       'output_layer': np.float64(0.624429272890091)},\n",
              "      'num_tokens': 27}},\n",
              "    {'prompt': 'output only the code, no explanation: Write a function to find whether a given array of integers contains any duplicate element.',\n",
              "     'ground_truth_code': 'def test_duplicate(arraynums):\\r\\n    nums_set = set(arraynums)    \\r\\n    return len(arraynums) != len(nums_set)     ',\n",
              "     'generated_code': ')\\n\\n the the number.\\n\\n no explanation or # a Python that compute the a given integer of integers contains exactly duplicate numbers(s Return',\n",
              "     'test_cases': ['assert test_duplicate(([1,2,3,4,5]))==False',\n",
              "      'assert test_duplicate(([1,2,3,4, 4]))==True',\n",
              "      'assert test_duplicate([1,1,2,2,3,3,4,4,5])==True'],\n",
              "     'is_correct': False,\n",
              "     'stats': {'total_energy': 26.96234369187057,\n",
              "      'tokenization_energy': 0.18934369182586672,\n",
              "      'inference_energy': 26.773000000044703,\n",
              "      'energy_per_token': 1.0784937476748226,\n",
              "      'time': 0.6263892650604248,\n",
              "      'components': {'embeddings': np.float64(0.1146602668762207),\n",
              "       'attention': np.float64(17.905325639261868),\n",
              "       'ffn': np.float64(5.691412503719329),\n",
              "       'layernorm': np.float64(0.1223869800567627),\n",
              "       'output_layer': np.float64(0.6267520904541016)},\n",
              "      'num_tokens': 25}},\n",
              "    {'prompt': 'output only the code, no explanation: Write a function to check if the given number is woodball or not.',\n",
              "     'ground_truth_code': 'def is_woodall(x): \\r\\n\\tif (x % 2 == 0): \\r\\n\\t\\treturn False\\r\\n\\tif (x == 1): \\r\\n\\t\\treturn True\\r\\n\\tx = x + 1 \\r\\n\\tp = 0\\r\\n\\twhile (x % 2 == 0): \\r\\n\\t\\tx = x/2\\r\\n\\t\\tp = p + 1\\r\\n\\t\\tif (p == x): \\r\\n\\t\\t\\treturn True\\r\\n\\treturn False',\n",
              "     'generated_code': ')\\n\\n the the number.\\n\\n no explanation or # a Python that compute if a number string is awork number not.\\n\\n The',\n",
              "     'test_cases': ['assert is_woodall(383) == True',\n",
              "      'assert is_woodall(254) == False',\n",
              "      'assert is_woodall(200) == False'],\n",
              "     'is_correct': False,\n",
              "     'stats': {'total_energy': 26.70278102113865,\n",
              "      'tokenization_energy': 0.15578102111816408,\n",
              "      'inference_energy': 26.54700000002049,\n",
              "      'energy_per_token': 1.1126158758807771,\n",
              "      'time': 0.6152706146240234,\n",
              "      'components': {'embeddings': np.float64(0.12992972946166992),\n",
              "       'attention': np.float64(4.448662375450135),\n",
              "       'ffn': np.float64(5.885220607757569),\n",
              "       'layernorm': np.float64(0.11336362361907959),\n",
              "       'output_layer': np.float64(4.4329999999608845)},\n",
              "      'num_tokens': 24}}],\n",
              "   'summary': {'avg_energy': 30.78570637661191,\n",
              "    'avg_time': 0.7880952835083008,\n",
              "    'energy_per_token': 1.2935170746475593,\n",
              "    'accuracy': 0.0,\n",
              "    'carbon_emissions': 9.406743615075861,\n",
              "    'total_examples': 10}}}}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# run the selected benchmark\n",
        "results = run_task(cfg)\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'prompt': 'who are ShakeSpear?',\n",
              " 'ground_truth_code': 'def remove_Occ(s,ch): \\r\\n    for i in range(len(s)): \\r\\n        if (s[i] == ch): \\r\\n            s = s[0 : i] + s[i + 1:] \\r\\n            break\\r\\n    for i in range(len(s) - 1,-1,-1):  \\r\\n        if (s[i] == ch): \\r\\n            s = s[0 : i] + s[i + 1:] \\r\\n            break\\r\\n    return s ',\n",
              " 'generated_code': \")\\n\\n is theypeare's What\",\n",
              " 'test_cases': ['assert remove_Occ(\"hello\",\"l\") == \"heo\"',\n",
              "  'assert remove_Occ(\"abcda\",\"a\") == \"bcd\"',\n",
              "  'assert remove_Occ(\"PHP\",\"P\") == \"H\"'],\n",
              " 'is_correct': False,\n",
              " 'stats': {'total_energy': 26.15790844726551,\n",
              "  'tokenization_energy': 0.104908447265625,\n",
              "  'inference_energy': 26.052999999999884,\n",
              "  'energy_per_token': 3.736844063895073,\n",
              "  'time': 0.8198516368865967,\n",
              "  'components': {'embeddings': np.float64(0.087492919921875),\n",
              "   'attention': np.float64(11.333255250458228),\n",
              "   'ffn': np.float64(14.213735697983967),\n",
              "   'layernorm': np.float64(0.1033801999092102),\n",
              "   'output_layer': np.float64(0.17254934811592101)},\n",
              "  'num_tokens': 7}}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results[\"mbpp\"][\"fp16\"][\"examples\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJ0vxxEMp7d_",
        "outputId": "44969c71-de3c-4064-c0e5-007e2b658557"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Location detected: Singapore, SG (lat: 1.2897, lon: 103.8501)\n",
            "Using estimated carbon intensity.\n",
            "No specific estimate for SG. Using global average: 475 gCO2eq/kWh\n",
            "\n",
            "=== MATH SUMMARY ===\n",
            "fp16_vanilla: E=16.09 J, Lat=0.501s, Acc=0.00%, CO₂=42.4599g\n",
            "int8_vanilla: E=13.51 J, Lat=0.500s, Acc=0.00%, CO₂=35.6470g\n",
            "int4_vanilla: E=18.01 J, Lat=0.548s, Acc=0.00%, CO₂=47.5221g\n"
          ]
        }
      ],
      "source": [
        "# print summary for each task and mode\n",
        "ci = get_carbon_intensity()\n",
        "for task, modes in results.items():\n",
        "    print(f\"\\n=== {task.upper()} SUMMARY ===\")\n",
        "    for mode, data in modes.items():\n",
        "        summary = data.get(\"summary\", data)\n",
        "        e   = summary.get(\"avg_energy\",     summary.get(\"total_energy\", 0.0))\n",
        "        t   = summary.get(\"avg_time\",       summary.get(\"total_time\",   0.0))\n",
        "        acc = summary.get(\"accuracy\",       None)\n",
        "        co2 = summary.get(\n",
        "            \"carbon_emissions\",\n",
        "            joules_to_co2(summary.get(\"total_energy\", e), ci)\n",
        "        )\n",
        "        line = f\"{mode:>12}: E={e:.2f} J, Lat={t:.3f}s\"\n",
        "        if acc is not None:\n",
        "            line += f\", Acc={acc:.2f}%\"\n",
        "        line += f\", CO₂={co2:.4f}g\"\n",
        "        print(line)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "Y63kgYMsp9Xm"
      },
      "outputs": [],
      "source": [
        "# Plot overall energy comparison\n",
        "plot_energy_comparison(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "Jo3ttZ9BjeKc"
      },
      "outputs": [],
      "source": [
        "# Plot per-component breakdown for each task and mode\n",
        "for task, modes in results.items():\n",
        "    for mode in modes:\n",
        "        # skip modes without component stats\n",
        "        stat = results[task][mode]\n",
        "        comps = stat.get(\"summary\", stat).get(\"components\", None)\n",
        "        if comps:\n",
        "            plot_component_energy(results, task_type=task, quant_mode=mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kum6MkQjqAH9",
        "outputId": "aaace621-7e63-4652-b33f-c243d86b32ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results saved to results.json\n"
          ]
        }
      ],
      "source": [
        "# save raw results to JSON\n",
        "with open(cfg[\"output_file\"], \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "print(f\"Results saved to {cfg['output_file']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FTTSkbxqBBI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.22"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
