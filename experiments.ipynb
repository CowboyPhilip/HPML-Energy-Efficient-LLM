{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fQVHjJD4j4ig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cfc610f-5206-448c-941a-8181eb244eb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run_experiment.py  sample_data\tutils\n"
          ]
        }
      ],
      "source": [
        "# run this cell if you are in colab with a single notebook opened, otherwise ignore this cell\n",
        "\n",
        "#!git clone https://github.com/CowboyPhilip/HPML-Energy-Efficient-LLM\n",
        "#%cd HPML-Energy-Efficient-LLM\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5G8E_rswj5Pf",
        "outputId": "1cfeca60-ad92-45e8-db8f-74a1a25b67be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (80.3.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
            "Requirement already satisfied: zeus-ml in /usr/local/lib/python3.11/dist-packages (0.11.0.post1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: geocoder in /usr/local/lib/python3.11/dist-packages (1.38.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from zeus-ml) (2.2.2)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.11/dist-packages (from zeus-ml) (12.570.86)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from zeus-ml) (2.11.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from zeus-ml) (13.9.4)\n",
            "Requirement already satisfied: tyro in /usr/local/lib/python3.11/dist-packages (from zeus-ml) (0.9.19)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from zeus-ml) (0.28.1)\n",
            "Requirement already satisfied: amdsmi in /usr/local/lib/python3.11/dist-packages (from zeus-ml) (6.4.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from zeus-ml) (2.9.0.post0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from geocoder) (8.1.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from geocoder) (1.0.0)\n",
            "Requirement already satisfied: ratelim in /usr/local/lib/python3.11/dist-packages (from geocoder) (0.1.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from geocoder) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.27.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (80.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->zeus-ml) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->zeus-ml) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->zeus-ml) (0.4.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->zeus-ml) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->zeus-ml) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->zeus-ml) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->zeus-ml) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->zeus-ml) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->zeus-ml) (2025.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ratelim->geocoder) (4.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->zeus-ml) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->zeus-ml) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->zeus-ml) (0.1.2)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->zeus-ml) (0.16)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro->zeus-ml) (1.7.2)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->zeus-ml) (4.4.2)\n"
          ]
        }
      ],
      "source": [
        "# 1. Install dependencies\n",
        "!pip install --upgrade pip setuptools\n",
        "!pip install \\\n",
        "    transformers \\\n",
        "    bitsandbytes \\\n",
        "    zeus-ml \\\n",
        "    torch \\\n",
        "    datasets \\\n",
        "    evaluate \\\n",
        "    scikit-learn \\\n",
        "    geocoder \\\n",
        "    requests \\\n",
        "    numpy \\\n",
        "    wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \\\n",
        "    flash-attn==2.0.5 \\\n",
        "    triton==2.0.0 \\\n",
        "    vllm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwXrXRkoUWck",
        "outputId": "ab910035-ddf5-4da3-c7c6-d724ae0b2f24"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flash-attn==2.0.5\n",
            "  Using cached flash_attn-2.0.5.tar.gz (2.3 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting triton==2.0.0\n",
            "  Using cached triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.8.5.post1-cp38-abi3-manylinux1_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn==2.0.5) (2.6.0+cu124)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn==2.0.5) (0.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from flash-attn==2.0.5) (24.2)\n",
            "Collecting ninja (from flash-attn==2.0.5)\n",
            "  Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0) (3.31.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0) (3.18.0)\n",
            "Collecting lit (from triton==2.0.0)\n",
            "  Using cached lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from vllm) (5.5.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from vllm) (5.9.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from vllm) (0.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from vllm) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from vllm) (4.67.1)\n",
            "Collecting blake3 (from vllm)\n",
            "  Using cached blake3-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from vllm) (9.0.0)\n",
            "Requirement already satisfied: transformers>=4.51.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.51.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[hf_xet]>=0.30.0->vllm) (0.30.2)\n",
            "Requirement already satisfied: tokenizers>=0.21.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from vllm) (5.29.4)\n",
            "Collecting fastapi>=0.115.0 (from fastapi[standard]>=0.115.0->vllm)\n",
            "  Using cached fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from vllm) (3.11.15)\n",
            "Requirement already satisfied: openai>=1.52.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.76.2)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.11.4)\n",
            "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from vllm) (11.2.1)\n",
            "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\n",
            "  Using cached prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting tiktoken>=0.6.0 (from vllm)\n",
            "  Using cached tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting lm-format-enforcer<0.11,>=0.10.11 (from vllm)\n",
            "  Using cached lm_format_enforcer-0.10.11-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting llguidance<0.8.0,>=0.7.9 (from vllm)\n",
            "  Using cached llguidance-0.7.19-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting outlines==0.1.11 (from vllm)\n",
            "  Using cached outlines-0.1.11-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting lark==1.2.2 (from vllm)\n",
            "  Using cached lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xgrammar==0.1.18 (from vllm)\n",
            "  Using cached xgrammar-0.1.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.13.2)\n",
            "Collecting partial-json-parser (from vllm)\n",
            "  Using cached partial_json_parser-0.2.1.1.post5-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting pyzmq>=25.0.0 (from vllm)\n",
            "  Using cached pyzmq-26.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.0 kB)\n",
            "Collecting msgspec (from vllm)\n",
            "  Using cached msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting gguf>=0.13.0 (from vllm)\n",
            "  Using cached gguf-0.16.3-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from vllm) (8.7.0)\n",
            "Collecting mistral_common>=1.5.4 (from mistral_common[opencv]>=1.5.4->vllm)\n",
            "  Using cached mistral_common-1.5.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.11.0.86)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from vllm) (6.0.2)\n",
            "Collecting compressed-tensors==0.9.3 (from vllm)\n",
            "  Using cached compressed_tensors-0.9.3-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting depyf==0.18.0 (from vllm)\n",
            "  Using cached depyf-0.18.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from vllm) (3.1.1)\n",
            "Collecting watchfiles (from vllm)\n",
            "  Using cached watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting python-json-logger (from vllm)\n",
            "  Using cached python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from vllm) (1.15.2)\n",
            "Collecting opentelemetry-sdk<1.27.0,>=1.26.0 (from vllm)\n",
            "  Using cached opentelemetry_sdk-1.26.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api<1.27.0,>=1.26.0 (from vllm)\n",
            "  Using cached opentelemetry_api-1.26.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp<1.27.0,>=1.26.0 (from vllm)\n",
            "  Using cached opentelemetry_exporter_otlp-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-semantic-conventions-ai<0.5.0,>=0.4.1 (from vllm)\n",
            "  Using cached opentelemetry_semantic_conventions_ai-0.4.6-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting numba==0.61.2 (from vllm)\n",
            "  Using cached numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting ray!=2.44.*,>=2.43.0 (from ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n",
            "  Using cached ray-2.45.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: torchaudio==2.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision==0.21.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.0+cu124)\n",
            "Collecting xformers==0.0.29.post2 (from vllm)\n",
            "  Using cached xformers-0.0.29.post2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting astor (from depyf==0.18.0->vllm)\n",
            "  Using cached astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from depyf==0.18.0->vllm) (0.3.8)\n",
            "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm)\n",
            "  Using cached llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
            "Collecting interegular (from outlines==0.1.11->vllm)\n",
            "  Using cached interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (3.1.6)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (1.6.0)\n",
            "Collecting diskcache (from outlines==0.1.11->vllm)\n",
            "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: referencing in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.36.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (4.23.0)\n",
            "Collecting pycountry (from outlines==0.1.11->vllm)\n",
            "  Using cached pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting airportsdata (from outlines==0.1.11->vllm)\n",
            "  Using cached airportsdata-20250224-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting outlines_core==0.1.26 (from outlines==0.1.11->vllm)\n",
            "  Using cached outlines_core-0.1.26-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.0.5) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.0.5) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.0.5) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.0.5) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.0.5) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.0.5) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.0.5) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.0.5) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.0.5) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.0.5) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.0.5) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.0.5) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.0.5) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.0.5) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.0.5) (12.4.127)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch (from flash-attn==2.0.5)\n",
            "  Using cached torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.8.5-cp38-abi3-manylinux1_x86_64.whl.metadata (14 kB)\n",
            "Collecting torch (from flash-attn==2.0.5)\n",
            "  Using cached torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting sympy>=1.13.3 (from torch->flash-attn==2.0.5)\n",
            "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting torch (from flash-attn==2.0.5)\n",
            "  Using cached torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "  Using cached torch-2.5.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "  Using cached torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==2.0.5) (1.13.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "INFO: pip is still looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch (from flash-attn==2.0.5)\n",
            "  Using cached torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "  Using cached torch-2.3.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting torch (from flash-attn==2.0.5)\n",
            "  Using cached torch-2.3.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "  Using cached torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting torch (from flash-attn==2.0.5)\n",
            "  Using cached torch-2.2.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached torch-2.2.0-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "  Using cached torch-2.1.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting nvidia-nccl-cu12==2.18.1 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting torch (from flash-attn==2.0.5)\n",
            "  Using cached torch-2.1.1-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "  Using cached torch-2.1.0-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "  Using cached torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch->flash-attn==2.0.5)\n",
            "  Using cached nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->flash-attn==2.0.5) (80.3.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->flash-attn==2.0.5) (0.45.1)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.8.4-cp38-abi3-manylinux1_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.11/dist-packages (from vllm) (24.0.1)\n",
            "  Using cached vllm-0.8.3-cp38-abi3-manylinux1_x86_64.whl.metadata (27 kB)\n",
            "Collecting xgrammar==0.1.17 (from vllm)\n",
            "  Using cached xgrammar-0.1.17-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting gguf==0.10.0 (from vllm)\n",
            "  Using cached gguf-0.10.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting compressed-tensors==0.9.2 (from vllm)\n",
            "  Using cached compressed_tensors-0.9.2-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting numba==0.61 (from vllm)\n",
            "  Using cached numba-0.61.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.8.2-cp38-abi3-manylinux1_x86_64.whl.metadata (27 kB)\n",
            "Collecting numpy<2.0.0 (from vllm)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting xgrammar==0.1.16 (from vllm)\n",
            "  Using cached xgrammar-0.1.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: numba==0.60.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.60.0)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.8.1-cp38-abi3-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "  Using cached vllm-0.8.0-cp38-abi3-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "  Using cached vllm-0.7.3-cp38-abi3-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting xgrammar==0.1.11 (from vllm)\n",
            "  Using cached xgrammar-0.1.11-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting compressed-tensors==0.9.1 (from vllm)\n",
            "  Using cached compressed_tensors-0.9.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting ray==2.40.0 (from ray[adag]==2.40.0->vllm)\n",
            "  Using cached ray-2.40.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (17 kB)\n",
            "Collecting torchaudio==2.5.1 (from vllm)\n",
            "  Using cached torchaudio-2.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting torchvision==0.20.1 (from vllm)\n",
            "  Using cached torchvision-0.20.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting xformers==0.0.28.post3 (from vllm)\n",
            "  Using cached xformers-0.0.28.post3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.7.2-cp38-abi3-manylinux1_x86_64.whl.metadata (12 kB)\n",
            "Collecting uvicorn[standard] (from vllm)\n",
            "  Using cached uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: nvidia-ml-py>=12.560.30 in /usr/local/lib/python3.11/dist-packages (from vllm) (12.570.86)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.7.1-cp38-abi3-manylinux1_x86_64.whl.metadata (12 kB)\n",
            "Collecting compressed-tensors==0.9.0 (from vllm)\n",
            "  Using cached compressed_tensors-0.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.7.0-cp38-abi3-manylinux1_x86_64.whl.metadata (12 kB)\n",
            "  Using cached vllm-0.6.6.post1-cp38-abi3-manylinux1_x86_64.whl.metadata (11 kB)\n",
            "Collecting compressed-tensors==0.8.1 (from vllm)\n",
            "  Using cached compressed_tensors-0.8.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.6.6-cp38-abi3-manylinux1_x86_64.whl.metadata (11 kB)\n",
            "  Using cached vllm-0.6.5-cp38-abi3-manylinux1_x86_64.whl.metadata (11 kB)\n",
            "  Using cached vllm-0.6.4.post1-cp38-abi3-manylinux1_x86_64.whl.metadata (10 kB)\n",
            "Collecting outlines<0.1,>=0.0.43 (from vllm)\n",
            "  Using cached outlines-0.0.46-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting compressed-tensors==0.8.0 (from vllm)\n",
            "  Using cached compressed_tensors-0.8.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.6.4-cp38-abi3-manylinux1_x86_64.whl.metadata (10 kB)\n",
            "Collecting lm-format-enforcer==0.10.6 (from vllm)\n",
            "  Using cached lm_format_enforcer-0.10.6-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.6.3.post1-cp38-abi3-manylinux1_x86_64.whl.metadata (10 kB)\n",
            "Collecting compressed-tensors==0.6.0 (from vllm)\n",
            "  Using cached compressed_tensors-0.6.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting torchvision==0.19 (from vllm)\n",
            "  Using cached torchvision-0.19.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "Collecting xformers==0.0.27.post2 (from vllm)\n",
            "  Using cached xformers-0.0.27.post2-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.6.3-cp38-abi3-manylinux1_x86_64.whl.metadata (10 kB)\n",
            "  Using cached vllm-0.6.2-cp38-abi3-manylinux1_x86_64.whl.metadata (2.4 kB)\n",
            "  Using cached vllm-0.6.1.post2-cp38-abi3-manylinux1_x86_64.whl.metadata (2.4 kB)\n",
            "Collecting gguf==0.9.1 (from vllm)\n",
            "  Using cached gguf-0.9.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting vllm-flash-attn==2.6.1 (from vllm)\n",
            "  Using cached vllm_flash_attn-2.6.1-cp311-cp311-manylinux1_x86_64.whl.metadata (476 bytes)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.6.1.post1-cp38-abi3-manylinux1_x86_64.whl.metadata (2.3 kB)\n",
            "  Using cached vllm-0.6.1-cp38-abi3-manylinux1_x86_64.whl.metadata (2.3 kB)\n",
            "  Using cached vllm-0.6.0-cp38-abi3-manylinux1_x86_64.whl.metadata (2.2 kB)\n",
            "  Using cached vllm-0.5.5-cp38-abi3-manylinux1_x86_64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from vllm) (0.11.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (from vllm) (0.13.1)\n",
            "  Using cached vllm-0.5.4-cp38-abi3-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting lm-format-enforcer==0.10.3 (from vllm)\n",
            "  Using cached lm_format_enforcer-0.10.3-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.5.3.post1-cp38-abi3-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting torchvision==0.18.1 (from vllm)\n",
            "  Using cached torchvision-0.18.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting xformers==0.0.27 (from vllm)\n",
            "  Using cached xformers-0.0.27-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting vllm-flash-attn==2.5.9.post1 (from vllm)\n",
            "  Using cached vllm_flash_attn-2.5.9.post1-cp311-cp311-manylinux1_x86_64.whl.metadata (482 bytes)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.5.3-cp38-abi3-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "  Using cached vllm-0.5.2-cp38-abi3-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "  Using cached vllm-0.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (8.2 kB)\n",
            "Collecting lm-format-enforcer==0.10.1 (from vllm)\n",
            "  Using cached lm_format_enforcer-0.10.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting outlines>=0.0.43 (from vllm)\n",
            "  Using cached outlines-0.2.3-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting torchvision==0.18.0 (from vllm)\n",
            "  Using cached torchvision-0.18.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting xformers==0.0.26.post1 (from vllm)\n",
            "  Using cached xformers-0.0.26.post1-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting vllm-flash-attn==2.5.9 (from vllm)\n",
            "  Using cached vllm_flash_attn-2.5.9-cp311-cp311-manylinux1_x86_64.whl.metadata (476 bytes)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.5.0.post1-cp311-cp311-manylinux1_x86_64.whl.metadata (8.1 kB)\n",
            "  Using cached vllm-0.5.0-cp311-cp311-manylinux1_x86_64.whl.metadata (8.3 kB)\n",
            "  Using cached vllm-0.4.3-cp311-cp311-manylinux1_x86_64.whl.metadata (7.8 kB)\n",
            "Collecting outlines==0.0.34 (from vllm)\n",
            "  Using cached outlines-0.0.34-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting vllm-flash-attn==2.5.8.post2 (from vllm)\n",
            "  Using cached vllm_flash_attn-2.5.8.post2-cp311-cp311-manylinux1_x86_64.whl.metadata (482 bytes)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.4.2-cp311-cp311-manylinux1_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting tiktoken==0.6.0 (from vllm)\n",
            "  Using cached tiktoken-0.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting lm-format-enforcer==0.9.8 (from vllm)\n",
            "  Using cached lm_format_enforcer-0.9.8-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting vllm-nccl-cu12<2.19,>=2.18 (from vllm)\n",
            "  Using cached vllm_nccl_cu12-2.18.1.0.4.0.tar.gz (6.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.4.1-cp311-cp311-manylinux1_x86_64.whl.metadata (8.9 kB)\n",
            "Collecting xformers==0.0.25 (from vllm)\n",
            "  Using cached xformers-0.0.25-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.4.0.post1-cp311-cp311-manylinux1_x86_64.whl.metadata (8.6 kB)\n",
            "Collecting xformers==0.0.23.post1 (from vllm)\n",
            "  Using cached xformers-0.0.23.post1-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting pynvml==11.5.0 (from vllm)\n",
            "  Using cached pynvml-11.5.0-py3-none-any.whl.metadata (7.8 kB)\n",
            "INFO: pip is looking at multiple versions of vllm to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (8.6 kB)\n",
            "  Using cached vllm-0.3.3-cp311-cp311-manylinux1_x86_64.whl.metadata (7.8 kB)\n",
            "  Using cached vllm-0.3.2-cp311-cp311-manylinux1_x86_64.whl.metadata (7.5 kB)\n",
            "Collecting aioprometheus[starlette] (from vllm)\n",
            "  Using cached aioprometheus-23.12.0-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.3.1-cp311-cp311-manylinux1_x86_64.whl.metadata (7.4 kB)\n",
            "  Using cached vllm-0.3.0-cp311-cp311-manylinux1_x86_64.whl.metadata (7.4 kB)\n",
            "INFO: pip is still looking at multiple versions of vllm to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached vllm-0.2.7-cp311-cp311-manylinux1_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting pydantic==1.10.13 (from vllm)\n",
            "  Using cached pydantic-1.10.13-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (149 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.2.6-cp311-cp311-manylinux1_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from vllm) (2.2.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from vllm) (18.1.0)\n",
            "  Using cached vllm-0.2.5-cp311-cp311-manylinux1_x86_64.whl.metadata (6.5 kB)\n",
            "Collecting xformers>=0.0.23 (from vllm)\n",
            "  Using cached xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.2.4-cp311-cp311-manylinux1_x86_64.whl.metadata (6.8 kB)\n",
            "  Using cached vllm-0.2.3-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "  Using cached vllm-0.2.2-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "  Using cached vllm-0.2.1.post1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.2 kB)\n",
            "Collecting xformers==0.0.22 (from vllm)\n",
            "  Using cached xformers-0.0.22-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting pydantic<2 (from vllm)\n",
            "  Using cached pydantic-1.10.22-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (154 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (8.1.8)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.51.1->vllm) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.51.1->vllm) (0.5.3)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm)\n",
            "  Using cached starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->outlines==0.1.11->vllm) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (2025.4.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (0.24.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->vllm) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->vllm) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->vllm) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->vllm) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->flash-attn==2.0.5) (1.3.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm) (0.16.0)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]->vllm)\n",
            "  Using cached httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]->vllm)\n",
            "  Using cached python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]->vllm)\n",
            "  Using cached uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm) (15.0.1)\n",
            "Using cached triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "Using cached torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\n",
            "Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "Using cached nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "Using cached nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "Using cached nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "Using cached nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "Using cached nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "Using cached nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "Using cached vllm-0.2.1.post1-cp311-cp311-manylinux1_x86_64.whl (28.7 MB)\n",
            "Using cached xformers-0.0.22-cp311-cp311-manylinux2014_x86_64.whl (211.6 MB)\n",
            "Using cached pydantic-1.10.22-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "Using cached ray-2.45.0-cp311-cp311-manylinux2014_x86_64.whl (68.4 MB)\n",
            "Using cached fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "Using cached starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "Using cached lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "Using cached uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "Using cached httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "Using cached python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Using cached uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "Using cached watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n",
            "Building wheels for collected packages: flash-attn\n",
            "\u001b[33m  DEPRECATION: Building 'flash-attn' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'flash-attn'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPZVbDIHPQUc",
        "outputId": "a303e975-6ee1-4a6e-a737-8cd5b9ba0014"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjy3475\u001b[0m (\u001b[33mHPML-Energy-Efficient-LLM\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IbU-A3ARw1k",
        "outputId": "141edec9-3a68-4f82-fec8-859250e8b8a8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[1mCurrent Settings\u001b[0m\n",
            "{\n",
            "  \"_extra_http_headers\": null,\n",
            "  \"_proxies\": null,\n",
            "  \"api_key\": null,\n",
            "  \"base_url\": \"https://api.wandb.ai\",\n",
            "  \"entity\": null,\n",
            "  \"git_remote\": \"origin\",\n",
            "  \"ignore_globs\": [],\n",
            "  \"organization\": null,\n",
            "  \"project\": null,\n",
            "  \"root_dir\": null,\n",
            "  \"section\": \"default\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import wandb\n",
        "\n",
        "# wandb.init(project=\"HPML-Energy-Efficient-LLM\", name=\"test-connection\")\n",
        "\n",
        "# wandb.log({\"test_value\": \"init wandb\"})\n",
        "\n",
        "# wandb.finish()"
      ],
      "metadata": {
        "id": "Fy6rvCvQSQcz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "YVOgToaUjT50"
      },
      "outputs": [],
      "source": [
        "# global configuration for experiments\n",
        "cfg = {\n",
        "    \"task\":           \"math\",\n",
        "    \"model\":          \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
        "    # test default (vanilla) kernel at fp16/int8/int4, plus adaptive switching\n",
        "    \"modes\":          [\n",
        "        \"fp32_vanilla\",    # FP16 + vanilla Transformer\n",
        "        \"fp16_vanilla\",    # FP16 + vanilla Transformer\n",
        "        \"int8_vanilla\",    # INT8 + vanilla\n",
        "        \"int4_vanilla\",    # INT4 + vanilla& low_mode\n",
        "    ],\n",
        "    # for adaptive mode: which two modes to switch between\n",
        "    # \"high_mode\":      \"fp16_vanilla\",\n",
        "    # \"low_mode\":       \"int8_vanilla\",\n",
        "\n",
        "    \"dataset_name\":   \"deepmind/math_dataset\",\n",
        "    \"dataset_config\": \"algebra__linear_1d\",\n",
        "    \"split\":          \"test\",\n",
        "    \"num_examples\":   20,\n",
        "\n",
        "    # \"subjects\":       [\"physics\",\"chemistry\"],\n",
        "    # \"quick\":          True,\n",
        "    # \"max_samples\":    500,\n",
        "\n",
        "    # \"glue_tasks\":     [\"sst2\",\"cola\"],\n",
        "    # \"batch_size\":     1,\n",
        "\n",
        "    \"verbose\":        True,\n",
        "    \"output_file\":    \"results.json\",\n",
        "    \"device_map\": \"cuda\"\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5e1v5JwvDfu"
      },
      "outputs": [],
      "source": [
        "cfg = {\n",
        "    \"task\":        \"math\",                              # only MATH\n",
        "    \"model\":       \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
        "\n",
        "\n",
        "    \"modes\":       [\"adaptive\"],                        # only adaptive\n",
        "    \"high_mode\":   \"fp16_vanilla\",                      # high precision = FP16 + vanilla\n",
        "    \"low_mode\":    \"int8_vanilla\",                      # low precision = INT8 + vanilla\n",
        "\n",
        "    # MATH dataset\n",
        "    \"dataset_name\":   \"deepmind/math_dataset\",\n",
        "    \"dataset_config\": \"algebra__linear_1d\",\n",
        "    \"split\":      \"test\",\n",
        "    \"num_examples\":   20,\n",
        "\n",
        "    \"verbose\":     True,\n",
        "    \"output_file\": \"adaptive_math_results.json\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9wki5wqKQHK4"
      },
      "outputs": [],
      "source": [
        "# exp for mbpp\n",
        "cfg = {\n",
        "    \"task\":           \"mbpp\",\n",
        "    # \"model\":          \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
        "    \"model\":       \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "    # test default (vanilla) kernel at fp16/int8/int4, plus adaptive switching\n",
        "    \"modes\":          [\n",
        "        \"fp16_flash-v2\",    # FP16 + flash attn v2 Transformer\n",
        "        # \"int8_vanilla\",    # INT8 + vanilla\n",
        "        # \"int4_vanilla\",    # INT4 + vanilla& low_mode\n",
        "    ],\n",
        "    # for adaptive mode: which two modes to switch between\n",
        "    # \"high_mode\":      \"fp16_vanilla\",\n",
        "    # \"low_mode\":       \"int8_vanilla\",\n",
        "\n",
        "    \"dataset_name\":   \"deepmind/math_dataset\",\n",
        "    \"dataset_config\": \"algebra__linear_1d\",\n",
        "    \"split\":          \"test\",\n",
        "    \"num_examples\":   10,\n",
        "    \"subjects\":       [\"physics\",\"chemistry\"],\n",
        "    \"quick\":          True,\n",
        "    \"max_samples\":    500,\n",
        "    \"glue_tasks\":     [\"sst2\",\"cola\"],\n",
        "    \"batch_size\":     1,\n",
        "    \"verbose\":        True,\n",
        "    \"output_file\":    \"results.json\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "yA9v4noz0XtJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0fd078d-227d-4d1b-cd13-89da1740413d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset  # ensure load_dataset is defined\n",
        "\n",
        "# benchmark functions\n",
        "from utils.test_generation import quick_test_generation, test_generation_MATH, test_generation_MBPP\n",
        "from utils.test_mmlu      import quick_test_mmlu, test_quantized_models_on_mmlu\n",
        "from utils.test_glue      import test_quantized_models_on_glue\n",
        "\n",
        "# energy & tracking\n",
        "from utils.energy_utils   import EnergyTracker, get_carbon_intensity, joules_to_co2\n",
        "from utils.memory_utils   import clean_memory\n",
        "\n",
        "# adaptive quant wrapper\n",
        "from utils.adaptive_quant      import AdaptiveQuantGenerator\n",
        "\n",
        "# plotting\n",
        "from utils.plot_utils    import plot_energy_comparison, plot_component_energy\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "1oM_9fRa1Qq5"
      },
      "outputs": [],
      "source": [
        "# Monkey-patch EnergyTracker to support `with tracker:` and save_results\n",
        "def _et_enter(self):\n",
        "    if getattr(self, 'zeus', None):\n",
        "        try:\n",
        "            self.zeus.begin_window('inference')\n",
        "            self.active_windows.add('inference')\n",
        "        except:\n",
        "            pass\n",
        "    self._enter_ts = time.time()\n",
        "    return self\n",
        "\n",
        "def _et_exit(self, exc_type, exc_val, exc_tb):\n",
        "    end_ts = time.time()\n",
        "    inf_e = 0\n",
        "    if getattr(self, 'zeus', None) and 'inference' in self.active_windows:\n",
        "        try:\n",
        "            m = self.zeus.end_window('inference')\n",
        "            inf_e = m.total_energy\n",
        "            self.active_windows.remove('inference')\n",
        "        except:\n",
        "            pass\n",
        "    elapsed = end_ts - getattr(self, '_enter_ts', end_ts)\n",
        "    comp = {k: np.sum(v) for k, v in self.comp_energy.items()}\n",
        "    self.stats = {\n",
        "        'total_energy': inf_e,\n",
        "        'time': elapsed,\n",
        "        'components': comp,\n",
        "        'num_tokens': None\n",
        "    }\n",
        "    return False\n",
        "\n",
        "def _save_results(self, extra_metrics):\n",
        "    if not hasattr(self, 'stats'):\n",
        "        self.stats = {}\n",
        "    self.stats.update(extra_metrics)\n",
        "\n",
        "EnergyTracker.__enter__    = _et_enter\n",
        "EnergyTracker.__exit__     = _et_exit\n",
        "EnergyTracker.save_results = _save_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "BgCZi3UfjVih"
      },
      "outputs": [],
      "source": [
        "def run_task(cfg):\n",
        "    \"\"\"Dispatch benchmarks based on cfg['task'].\"\"\"\n",
        "    task = cfg[\"task\"]\n",
        "    modes = list(cfg[\"modes\"])\n",
        "    results = {}\n",
        "\n",
        "    # skip adaptive for pure classification tasks\n",
        "    if task in (\"glue\", \"mmlu\") and \"adaptive\" in modes:\n",
        "        print(\"⚠️  Skipping adaptive for classification tasks\")\n",
        "        modes.remove(\"adaptive\")\n",
        "\n",
        "    # text generation benchmark\n",
        "    if task == \"generation\":\n",
        "        results[\"generation\"] = {}\n",
        "        # adaptive mode\n",
        "        if \"adaptive\" in modes:\n",
        "            print(\"\\n=== ADAPTIVE generation ===\")\n",
        "            agent = AdaptiveQuantGenerator(\n",
        "                cfg[\"model\"],\n",
        "                high_mode=cfg[\"high_mode\"],\n",
        "                low_mode=cfg[\"low_mode\"]\n",
        "            )\n",
        "            _ = agent.generate(cfg[\"prompt\"], max_new_tokens=cfg[\"tokens\"])\n",
        "            results[\"generation\"][\"adaptive\"] = {\"note\": \"see adaptive_quant logs\"}\n",
        "            modes.remove(\"adaptive\")\n",
        "        # other quant/kernel modes\n",
        "        for mode in modes:\n",
        "            print(f\"\\n=== {mode.upper()} generation ===\")\n",
        "            stats = quick_test_generation(\n",
        "                model_name=cfg[\"model\"],\n",
        "                quant_mode=mode,\n",
        "                prompt=cfg[\"prompt\"],\n",
        "                max_new_tokens=cfg[\"tokens\"]\n",
        "            )\n",
        "            results[\"generation\"][mode] = stats\n",
        "\n",
        "    # MATH dataset benchmark (generation-style)\n",
        "    elif task == \"math\":\n",
        "        results[\"math\"] = {}\n",
        "        # adaptive on MATH\n",
        "        if \"adaptive\" in modes:\n",
        "            print(\"\\n=== ADAPTIVE on MATH ===\")\n",
        "            ds = load_dataset(\n",
        "                cfg[\"dataset_name\"],\n",
        "                cfg[\"dataset_config\"],\n",
        "                split=cfg[\"split\"]\n",
        "            ).select(range(cfg[\"num_examples\"]))\n",
        "            adapter = AdaptiveQuantGenerator(\n",
        "                cfg[\"model\"],\n",
        "                high_mode=cfg[\"high_mode\"],\n",
        "                low_mode=cfg[\"low_mode\"]\n",
        "            )\n",
        "            examples, correct, total_tokens = [], 0, 0\n",
        "            for ex in tqdm(ds, desc=\"Adaptive MATH\"):\n",
        "                q, ans = ex[\"question\"], ex[\"answer\"].strip()\n",
        "                tracker = EnergyTracker(\"adaptive_quant\")\n",
        "                with tracker:\n",
        "                    out = adapter.generate(q, max_new_tokens=32)\n",
        "                pred = out.strip()\n",
        "                correct += int(pred == ans)\n",
        "                total_tokens += tracker.stats.get(\"tokens_generated\", 1)\n",
        "                examples.append({\n",
        "                    \"question\": q,\n",
        "                    \"prediction\": pred,\n",
        "                    \"is_correct\": pred == ans,\n",
        "                    \"stats\": tracker.stats\n",
        "                })\n",
        "                clean_memory()\n",
        "            n = len(examples)\n",
        "            total_e = sum(e[\"stats\"][\"total_energy\"] for e in examples)\n",
        "            total_t = sum(e[\"stats\"][\"time\"]         for e in examples)\n",
        "            results[\"math\"][\"adaptive\"] = {\n",
        "                \"examples\": examples,\n",
        "                \"summary\": {\n",
        "                    \"accuracy\":         100 * correct / n,\n",
        "                    \"avg_energy\":       total_e / n,\n",
        "                    \"avg_time\":         total_t / n,\n",
        "                    \"energy_per_token\": total_e / total_tokens,\n",
        "                    \"carbon_emissions\": joules_to_co2(total_e, get_carbon_intensity())\n",
        "                }\n",
        "            }\n",
        "            plot_component_energy(results, task_type=\"math\", quant_mode=\"adaptive\")\n",
        "            modes.remove(\"adaptive\")\n",
        "\n",
        "        # standard quant modes on MATH\n",
        "        if modes:\n",
        "            print(f\"\\n=== standard modes on MATH: {modes} ===\")\n",
        "            std = test_generation_MATH(\n",
        "                model_name=cfg[\"model\"],\n",
        "                quantization_modes=modes,\n",
        "                dataset_name=cfg[\"dataset_name\"],\n",
        "                dataset_config=cfg[\"dataset_config\"],\n",
        "                split=cfg[\"split\"],\n",
        "                num_examples=cfg[\"num_examples\"],\n",
        "                verbose=cfg[\"verbose\"]\n",
        "            )\n",
        "            results[\"math\"].update(std)\n",
        "\n",
        "    # MBPP dataset benchmark\n",
        "    elif task == \"mbpp\":\n",
        "        print(\"\\n=== MBPP task ===\")\n",
        "        results[\"mbpp\"] = test_generation_MBPP(\n",
        "            model_name=cfg[\"model\"],\n",
        "            quantization_modes=modes,\n",
        "            num_examples=cfg[\"num_examples\"],\n",
        "            verbose=cfg[\"verbose\"]\n",
        "        )\n",
        "\n",
        "    # MMLU multiple-choice benchmark\n",
        "    elif task == \"mmlu\":\n",
        "        print(\"\\n=== MMLU task ===\")\n",
        "        if cfg.get(\"quick\", False):\n",
        "            stats = quick_test_mmlu(\n",
        "                model_name=cfg[\"model\"],\n",
        "                quant_mode=modes[0],\n",
        "                subjects=cfg[\"subjects\"],\n",
        "                max_samples=cfg[\"max_samples\"]\n",
        "            )\n",
        "        else:\n",
        "            stats = test_quantized_models_on_mmlu(\n",
        "                model_name=cfg[\"model\"],\n",
        "                quantization_modes=modes,\n",
        "                subjects=cfg[\"subjects\"]\n",
        "            )\n",
        "        results[\"mmlu\"] = stats\n",
        "\n",
        "    # GLUE classification benchmark\n",
        "    else:\n",
        "        print(\"\\n=== GLUE task ===\")\n",
        "        results[\"glue\"] = test_quantized_models_on_glue(\n",
        "            model_name=cfg[\"model\"],\n",
        "            tasks=cfg[\"glue_tasks\"],\n",
        "            quantization_modes=modes,\n",
        "            batch_size=cfg[\"batch_size\"]\n",
        "        )\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "wandb.init(\n",
        "    project=\"HPML-Energy-Efficient-LLM\",\n",
        "    name=f\"{cfg['model']}-{cfg['task']}-{'-'.join(cfg['modes'])}\",\n",
        "    tags=[cfg['model'].split('/')[-1], cfg['task']] + cfg['modes'],\n",
        "    group=cfg['model'].split('/')[-1],\n",
        "    job_type=cfg['task'],\n",
        "    config=cfg\n",
        ")\n",
        "\n",
        "results = run_task(cfg)\n",
        "\n",
        "wandb.log(results)\n",
        "wandb.finish()\n",
        "print(f\"\\nSaved results to wandb\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Z5NwsmSuTlcb",
        "outputId": "9d7908e8-0a7f-4fcf-dc74-2c6a605e2ce7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250507_012619-1zcflhyq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/HPML-Energy-Efficient-LLM/HPML-Energy-Efficient-LLM/runs/1zcflhyq' target=\"_blank\">deepseek-ai/deepseek-coder-1.3b-instruct-math-fp32_vanilla-fp16_vanilla-int8_vanilla-int4_vanilla</a></strong> to <a href='https://wandb.ai/HPML-Energy-Efficient-LLM/HPML-Energy-Efficient-LLM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/HPML-Energy-Efficient-LLM/HPML-Energy-Efficient-LLM' target=\"_blank\">https://wandb.ai/HPML-Energy-Efficient-LLM/HPML-Energy-Efficient-LLM</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/HPML-Energy-Efficient-LLM/HPML-Energy-Efficient-LLM/runs/1zcflhyq' target=\"_blank\">https://wandb.ai/HPML-Energy-Efficient-LLM/HPML-Energy-Efficient-LLM/runs/1zcflhyq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== standard modes on MATH: ['fp32_vanilla', 'fp16_vanilla', 'int8_vanilla', 'int4_vanilla'] ===\n",
            "Location detected: Las Vegas, US (lat: 36.175, lon: -115.1372)\n",
            "Using estimated carbon intensity.\n",
            "Using estimate for US: 417 gCO2eq/kWh\n",
            "Carbon intensity: 417 gCO2eq/kWh\n",
            "\n",
            "=== Testing FP32_VANILLA on MATH ===\n",
            "Loading FP32 model …\n",
            "GPU Memory: Allocated: 5.40 GB | Reserved: 10.78 GB | Max: 5.43 GB\n",
            "Model ready → quantisation: FP32, kernel: vanilla\n",
            "GPU Memory: Allocated: 10.79 GB | Reserved: 11.09 GB | Max: 10.80 GB\n",
            "[2025-05-07 01:26:24,314] [zeus.monitor.energy](energy.py:209) Monitoring GPU indices [0].\n",
            "[2025-05-07 01:26:24,315] [zeus.monitor.energy](energy.py:210) Monitoring CPU indices []\n",
            "Successfully initialized ZeusMonitor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MATH FP32_VANILLA: 100%|██████████| 20/20 [00:09<00:00,  2.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FP32_VANILLA SUMMARY:\n",
            "  Samples       : 20\n",
            "  Accuracy      : 0.00%\n",
            "  Energy/Infer  : 18.2258 J\n",
            "  Time/Infer    : 0.480 s\n",
            "  Energy/Token  : 0.542434 J/token\n",
            "  CO2 Emissions : 42.223072 gCO2eq\n",
            "\n",
            "=== Testing FP16_VANILLA on MATH ===\n",
            "Loading FP16 model …\n",
            "GPU Memory: Allocated: 5.41 GB | Reserved: 10.78 GB | Max: 10.82 GB\n",
            "Model ready → quantisation: FP16, kernel: vanilla\n",
            "GPU Memory: Allocated: 8.10 GB | Reserved: 10.78 GB | Max: 10.82 GB\n",
            "[2025-05-07 01:26:36,200] [zeus.monitor.energy](energy.py:209) Monitoring GPU indices [0].\n",
            "[2025-05-07 01:26:36,200] [zeus.monitor.energy](energy.py:210) Monitoring CPU indices []\n",
            "Successfully initialized ZeusMonitor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MATH FP16_VANILLA: 100%|██████████| 20/20 [00:10<00:00,  1.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FP16_VANILLA SUMMARY:\n",
            "  Samples       : 20\n",
            "  Accuracy      : 0.00%\n",
            "  Energy/Infer  : 21.5509 J\n",
            "  Time/Infer    : 0.533 s\n",
            "  Energy/Token  : 0.641394 J/token\n",
            "  CO2 Emissions : 49.926146 gCO2eq\n",
            "\n",
            "=== Testing INT8_VANILLA on MATH ===\n",
            "Loading INT8 model …\n",
            "GPU Memory: Allocated: 5.41 GB | Reserved: 10.78 GB | Max: 10.82 GB\n",
            "Model ready → quantisation: INT8, kernel: vanilla\n",
            "GPU Memory: Allocated: 6.89 GB | Reserved: 10.79 GB | Max: 10.82 GB\n",
            "[2025-05-07 01:26:50,451] [zeus.monitor.energy](energy.py:209) Monitoring GPU indices [0].\n",
            "[2025-05-07 01:26:50,452] [zeus.monitor.energy](energy.py:210) Monitoring CPU indices []\n",
            "Successfully initialized ZeusMonitor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MATH INT8_VANILLA: 100%|██████████| 20/20 [00:09<00:00,  2.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "INT8_VANILLA SUMMARY:\n",
            "  Samples       : 20\n",
            "  Accuracy      : 0.00%\n",
            "  Energy/Infer  : 17.2849 J\n",
            "  Time/Infer    : 0.493 s\n",
            "  Energy/Token  : 0.514431 J/token\n",
            "  CO2 Emissions : 40.043328 gCO2eq\n",
            "\n",
            "=== Testing INT4_VANILLA on MATH ===\n",
            "Loading INT4 model …\n",
            "GPU Memory: Allocated: 5.41 GB | Reserved: 10.78 GB | Max: 10.82 GB\n",
            "Model ready → quantisation: INT4, kernel: vanilla\n",
            "GPU Memory: Allocated: 6.30 GB | Reserved: 10.80 GB | Max: 10.82 GB\n",
            "[2025-05-07 01:27:04,052] [zeus.monitor.energy](energy.py:209) Monitoring GPU indices [0].\n",
            "[2025-05-07 01:27:04,054] [zeus.monitor.energy](energy.py:210) Monitoring CPU indices []\n",
            "Successfully initialized ZeusMonitor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MATH INT4_VANILLA: 100%|██████████| 20/20 [00:11<00:00,  1.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "INT4_VANILLA SUMMARY:\n",
            "  Samples       : 20\n",
            "  Accuracy      : 0.00%\n",
            "  Energy/Infer  : 22.4808 J\n",
            "  Time/Infer    : 0.558 s\n",
            "  Energy/Token  : 0.669071 J/token\n",
            "  CO2 Emissions : 52.080473 gCO2eq\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">deepseek-ai/deepseek-coder-1.3b-instruct-math-fp32_vanilla-fp16_vanilla-int8_vanilla-int4_vanilla</strong> at: <a href='https://wandb.ai/HPML-Energy-Efficient-LLM/HPML-Energy-Efficient-LLM/runs/1zcflhyq' target=\"_blank\">https://wandb.ai/HPML-Energy-Efficient-LLM/HPML-Energy-Efficient-LLM/runs/1zcflhyq</a><br> View project at: <a href='https://wandb.ai/HPML-Energy-Efficient-LLM/HPML-Energy-Efficient-LLM' target=\"_blank\">https://wandb.ai/HPML-Energy-Efficient-LLM/HPML-Energy-Efficient-LLM</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250507_012619-1zcflhyq/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved results to wandb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHcEn3-pQHK6",
        "outputId": "60f92b0e-e706-4c25-a72f-cf399e7be52b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': \"b'Solve -282*d + 929 - 178 = -1223 for d.\\\\n'\",\n",
              " 'ground_truth': \"b'7\\\\n'\",\n",
              " 'prediction': \"erer\\nve for>x.3x219** 102* -1105\\\\ the inn'\",\n",
              " 'is_correct': False,\n",
              " 'stats': {'total_energy': 18.70159947586525,\n",
              "  'tokenization_energy': 0.0715994758605957,\n",
              "  'inference_energy': 18.630000000004657,\n",
              "  'energy_per_token': 0.6032774024472661,\n",
              "  'time': 0.45252323150634766,\n",
              "  'components': {'embeddings': np.float64(0.07191507029533387),\n",
              "   'attention': np.float64(9.929176444523968),\n",
              "   'ffn': np.float64(7.49257587741781),\n",
              "   'layernorm': np.float64(0.077672860622406),\n",
              "   'output_layer': np.float64(0.1590070080757141)},\n",
              "  'num_tokens': 31}}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "results[\"math\"][\"fp32_vanilla\"][\"examples\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJ0vxxEMp7d_",
        "outputId": "de7193c5-e632-493f-eb9e-2f824504de7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Location detected: Las Vegas, US (lat: 36.175, lon: -115.1372)\n",
            "Using estimated carbon intensity.\n",
            "Using estimate for US: 417 gCO2eq/kWh\n",
            "\n",
            "=== MATH SUMMARY ===\n",
            "fp32_vanilla: E=18.23 J, Lat=0.480s, CO₂=42.2231g\n",
            "fp16_vanilla: E=21.55 J, Lat=0.533s, CO₂=49.9261g\n",
            "int8_vanilla: E=17.28 J, Lat=0.493s, CO₂=40.0433g\n",
            "int4_vanilla: E=22.48 J, Lat=0.558s, CO₂=52.0805g\n"
          ]
        }
      ],
      "source": [
        "# print summary for each task and mode\n",
        "ci = get_carbon_intensity()\n",
        "for task, modes in results.items():\n",
        "    print(f\"\\n=== {task.upper()} SUMMARY ===\")\n",
        "    for mode, data in modes.items():\n",
        "        summary = data.get(\"summary\", data)\n",
        "        e   = summary.get(\"avg_energy\",     summary.get(\"total_energy\", 0.0))\n",
        "        t   = summary.get(\"avg_time\",       summary.get(\"total_time\",   0.0))\n",
        "        # acc = summary.get(\"accuracy\",       None)\n",
        "        co2 = summary.get(\n",
        "            \"carbon_emissions\",\n",
        "            joules_to_co2(summary.get(\"total_energy\", e), ci)\n",
        "        )\n",
        "        line = f\"{mode:>12}: E={e:.2f} J, Lat={t:.3f}s\"\n",
        "        # if acc is not None:\n",
        "        #     line += f\", Acc={acc:.2f}%\"\n",
        "        line += f\", CO₂={co2:.4f}g\"\n",
        "        print(line)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Y63kgYMsp9Xm"
      },
      "outputs": [],
      "source": [
        "# Plot overall energy comparison\n",
        "plot_energy_comparison(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Jo3ttZ9BjeKc"
      },
      "outputs": [],
      "source": [
        "# Plot per-component breakdown for each task and mode\n",
        "for task, modes in results.items():\n",
        "    for mode in modes:\n",
        "        # skip modes without component stats\n",
        "        stat = results[task][mode]\n",
        "        comps = stat.get(\"summary\", stat).get(\"components\", None)\n",
        "        if comps:\n",
        "            plot_component_energy(results, task_type=task, quant_mode=mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kum6MkQjqAH9",
        "outputId": "cf2f5df3-f22c-412a-d358-a33272f929c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to results.json\n"
          ]
        }
      ],
      "source": [
        "# save raw results to JSON\n",
        "with open(cfg[\"output_file\"], \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "print(f\"Results saved to {cfg['output_file']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FTTSkbxqBBI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.22"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}