{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQVHjJD4j4ig",
        "outputId": "91d8f954-0966-474a-a009-428a4ee0790d"
      },
      "outputs": [],
      "source": [
        "# run this cell if you are in colab with a single notebook opened, otherwise ignore this cell\n",
        "\n",
        "#!git clone https://github.com/CowboyPhilip/HPML-Energy-Efficient-LLM\n",
        "#%cd HPML-Energy-Efficient-LLM\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5G8E_rswj5Pf",
        "outputId": "af2df541-d972-4511-a253-d67c3f4acce2"
      },
      "outputs": [],
      "source": [
        "# 1. Install dependencies\n",
        "!pip install --upgrade pip setuptools\n",
        "!pip install \\\n",
        "    transformers \\\n",
        "    bitsandbytes \\\n",
        "    zeus-ml \\\n",
        "    torch \\\n",
        "    datasets \\\n",
        "    evaluate \\\n",
        "    scikit-learn \\\n",
        "    geocoder \\\n",
        "    requests \\\n",
        "    numpy \\\n",
        "    wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwXrXRkoUWck",
        "outputId": "ab910035-ddf5-4da3-c7c6-d724ae0b2f24"
      },
      "outputs": [],
      "source": [
        "!pip install \\\n",
        "    flash-attn==2.0.5 \\\n",
        "    triton==2.0.0 \\\n",
        "    vllm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPZVbDIHPQUc",
        "outputId": "11423099-886f-46bb-a2fa-27e5f2240fbc"
      },
      "outputs": [],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IbU-A3ARw1k",
        "outputId": "3d4567ee-17ae-493e-bd4c-21c73c96d196"
      },
      "outputs": [],
      "source": [
        "!wandb status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fy6rvCvQSQcz"
      },
      "outputs": [],
      "source": [
        "# import wandb\n",
        "\n",
        "# wandb.init(project=\"HPML-Energy-Efficient-LLM\", name=\"test-connection\")\n",
        "\n",
        "# wandb.log({\"test_value\": \"init wandb\"})\n",
        "\n",
        "# wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVOgToaUjT50"
      },
      "outputs": [],
      "source": [
        "# global configuration for experiments\n",
        "cfg = {\n",
        "    \"task\":           \"math\",\n",
        "    \"model\":          \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
        "    # test adaptive quant\n",
        "    \"modes\": [\n",
        "        \"adaptive\"\n",
        "        # \"fp32_vanilla\",    # FP16 + vanilla Transformer\n",
        "        # \"fp16_vanilla\",    # FP16 + vanilla Transformer\n",
        "        # \"int8_vanilla\",    # INT8 + vanilla\n",
        "        # \"int4_vanilla\",    # INT4 + vanilla&\n",
        "    ],\n",
        "    # for adaptive mode: which two modes to switch between\n",
        "    \"high_mode\":      \"fp16_vanilla\",\n",
        "    \"low_mode\":       \"int8_vanilla\",\n",
        "    \"ctx_threshold\": 512,\n",
        "    \"latency_threshold\": 0.08,\n",
        "\n",
        "    \"dataset_name\":   \"deepmind/math_dataset\",\n",
        "    \"dataset_config\": \"algebra__linear_1d\",\n",
        "    \"split\":          \"test\",\n",
        "    \"num_examples\":   20,\n",
        "\n",
        "    \"temperature\": 0.5,\n",
        "    \"top_p\":0.9,\n",
        "    \"verbose\":        True,\n",
        "    \"output_file\":    \"results.json\",\n",
        "    \"device_map\": \"cuda\"\n",
        "\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5e1v5JwvDfu"
      },
      "outputs": [],
      "source": [
        "# global configuration for experiments\n",
        "cfg = {\n",
        "    \"task\":           \"math\",\n",
        "    \"model\":          \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
        "    # test default (vanilla) kernel\n",
        "    \"modes\": [\n",
        "        \"fp32_vanilla\",    # FP16 + vanilla Transformer\n",
        "        \"fp16_vanilla\",    # FP16 + vanilla Transformer\n",
        "        \"int8_vanilla\",    # INT8 + vanilla\n",
        "        \"int4_vanilla\",    # INT4 + vanilla&\n",
        "    ],\n",
        "\n",
        "    \"dataset_name\":   \"deepmind/math_dataset\",\n",
        "    \"dataset_config\": \"algebra__linear_1d\",\n",
        "    \"split\":          \"test\",\n",
        "    \"num_examples\":   20,\n",
        "\n",
        "    \"temperature\": 0.5,\n",
        "    \"top_p\":0.9,\n",
        "    \"verbose\":        True,\n",
        "    \"output_file\":    \"results.json\",\n",
        "    \"device_map\": \"cuda\"\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "9wki5wqKQHK4"
      },
      "outputs": [],
      "source": [
        "# exp for mbpp\n",
        "cfg = {\n",
        "    \"task\":           \"mbpp\",\n",
        "    \"model\":          \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
        "    # \"model\":       \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "    # test default (vanilla) kernel at fp16/int8/int4, plus adaptive switching\n",
        "    \"modes\":          [\n",
        "        # \"fp16_flash-v2\",    # FP16 + flash attn v2 Transformer, cannot use on t4\n",
        "        # \"fp16\",\n",
        "        # \"int8_vanilla\",    # INT8 + vanilla\n",
        "        \"int4_vanilla\",    # INT4 + vanilla& low_mode\n",
        "        \"adaptive\"\n",
        "    ],\n",
        "    # for adaptive mode: which two modes to switch between\n",
        "    # \"high_mode\":      \"fp16_vanilla\",\n",
        "    # \"low_mode\":       \"int8_vanilla\",\n",
        "    \"dataset_name\": \"mbpp\",\n",
        "    \"temperature\": 0,\n",
        "    \"top_p\":0.95,\n",
        "    \"num_examples\":   1,\n",
        "    \"quick\":          True,\n",
        "    \"max_samples\":    500,\n",
        "    \"batch_size\":     8,\n",
        "    \"verbose\":        True,\n",
        "    \"output_file\":    \"mbpp_results.json\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yA9v4noz0XtJ",
        "outputId": "82dd5385-0c07-4304-d6f4-0da250ad0084"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/vLLM/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/opt/rocm/lib/libamd_smi.so: cannot open shared object file: No such file or directory\n",
            "Unable to find libamd_smi.so library try installing amd-smi-lib from your package manager\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset  # ensure load_dataset is defined\n",
        "\n",
        "# benchmark functions\n",
        "from utils.test_generation import quick_test_generation, test_generation_MATH, test_generation_MBPP\n",
        "from utils.test_mmlu    import quick_test_mmlu, test_quantized_models_on_mmlu\n",
        "from utils.test_glue    import test_quantized_models_on_glue\n",
        "\n",
        "# energy & tracking\n",
        "from utils.energy_utils   import EnergyTracker, get_carbon_intensity, joules_to_co2\n",
        "from utils.memory_utils   import clean_memory\n",
        "\n",
        "# adaptive quant wrapper\n",
        "from utils.adaptive_quant  import AdaptiveQuantGenerator\n",
        "\n",
        "# plotting\n",
        "from utils.plot_utils    import plot_energy_comparison, plot_component_energy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1oM_9fRa1Qq5"
      },
      "outputs": [],
      "source": [
        "# Monkey-patch EnergyTracker to support `with tracker:` and save_results\n",
        "def _et_enter(self):\n",
        "    if getattr(self, 'zeus', None):\n",
        "        try:\n",
        "            self.zeus.begin_window('inference')\n",
        "            self.active_windows.add('inference')\n",
        "        except:\n",
        "            pass\n",
        "    self._enter_ts = time.time()\n",
        "    return self\n",
        "\n",
        "def _et_exit(self, exc_type, exc_val, exc_tb):\n",
        "    end_ts = time.time()\n",
        "    inf_e = 0\n",
        "    if getattr(self, 'zeus', None) and 'inference' in self.active_windows:\n",
        "        try:\n",
        "            m = self.zeus.end_window('inference')\n",
        "            inf_e = m.total_energy\n",
        "            self.active_windows.remove('inference')\n",
        "        except:\n",
        "            pass\n",
        "    elapsed = end_ts - getattr(self, '_enter_ts', end_ts)\n",
        "    comp = {k: np.sum(v) for k, v in self.comp_energy.items()}\n",
        "    self.stats = {\n",
        "        'total_energy': inf_e,\n",
        "        'time': elapsed,\n",
        "        'components': comp,\n",
        "        'num_tokens': None\n",
        "    }\n",
        "    return False\n",
        "\n",
        "def _save_results(self, extra_metrics):\n",
        "    if not hasattr(self, 'stats'):\n",
        "        self.stats = {}\n",
        "    self.stats.update(extra_metrics)\n",
        "\n",
        "EnergyTracker.__enter__    = _et_enter\n",
        "EnergyTracker.__exit__     = _et_exit\n",
        "EnergyTracker.save_results = _save_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "BgCZi3UfjVih"
      },
      "outputs": [],
      "source": [
        "def run_task(cfg):\n",
        "    \"\"\"Dispatch benchmarks based on cfg['task'], with adaptive-quant support.\"\"\"\n",
        "    task = cfg[\"task\"]\n",
        "    modes = list(cfg[\"modes\"])\n",
        "    results = {}\n",
        "\n",
        "    # unpack shared params\n",
        "    high_mode  = cfg.get(\"high_mode\")\n",
        "    low_mode   = cfg.get(\"low_mode\")\n",
        "    ctx_th     = cfg.get(\"ctx_threshold\", 1024)\n",
        "    lat_th     = cfg.get(\"latency_threshold\", 0.08)\n",
        "    device_map = cfg.get(\"device_map\", \"auto\")\n",
        "    temp       = cfg.get(\"temperature\", 0.5)\n",
        "    top_p      = cfg.get(\"top_p\", 0.9)\n",
        "    carbon_int = get_carbon_intensity()\n",
        "\n",
        "    # skip adaptive for pure classification\n",
        "    if task in (\"glue\", \"mmlu\") and \"adaptive\" in modes:\n",
        "        print(\"⚠️  Skipping adaptive for classification tasks\")\n",
        "        modes.remove(\"adaptive\")\n",
        "\n",
        "    # -------------------------\n",
        "    # TEXT GENERATION\n",
        "    # -------------------------\n",
        "    if task == \"generation\":\n",
        "        results[\"generation\"] = {}\n",
        "\n",
        "        # adaptive generation\n",
        "        if \"adaptive\" in modes:\n",
        "            print(\"\\n=== ADAPTIVE generation ===\")\n",
        "            agent = AdaptiveQuantGenerator(\n",
        "                cfg[\"model\"],\n",
        "                high_mode=high_mode,\n",
        "                low_mode=low_mode,\n",
        "                ctx_threshold=ctx_th,\n",
        "                latency_threshold=lat_th,\n",
        "                device_map=device_map\n",
        "            )\n",
        "            _, _, stats = agent.generate(\n",
        "                cfg[\"prompt\"],\n",
        "                max_new_tokens=cfg[\"tokens\"],\n",
        "                temperature=temp,\n",
        "                top_p=top_p\n",
        "            )\n",
        "            results[\"generation\"][\"adaptive\"] = stats\n",
        "            modes.remove(\"adaptive\")\n",
        "\n",
        "        # static generation modes\n",
        "        for mode in modes:\n",
        "            print(f\"\\n=== {mode.upper()} generation ===\")\n",
        "            stats = quick_test_generation(\n",
        "                model_name=cfg[\"model\"],\n",
        "                quant_mode=mode,\n",
        "                prompt=cfg[\"prompt\"],\n",
        "                max_new_tokens=cfg[\"tokens\"],\n",
        "                temperature=temp,\n",
        "                top_p=top_p\n",
        "            )\n",
        "            results[\"generation\"][mode] = stats\n",
        "\n",
        "    # -------------------------\n",
        "    # MATH (generation-style)\n",
        "    # -------------------------\n",
        "    elif task == \"math\":\n",
        "        results[\"math\"] = {}\n",
        "\n",
        "        # adaptive MATH\n",
        "        if \"adaptive\" in modes:\n",
        "            print(\"\\n=== ADAPTIVE on MATH ===\")\n",
        "            ds = load_dataset(\n",
        "                cfg[\"dataset_name\"],\n",
        "                cfg[\"dataset_config\"],\n",
        "                split=cfg[\"split\"]\n",
        "            ).select(range(cfg[\"num_examples\"]))\n",
        "\n",
        "            agent = AdaptiveQuantGenerator(\n",
        "                cfg[\"model\"],\n",
        "                high_mode=high_mode,\n",
        "                low_mode=low_mode,\n",
        "                ctx_threshold=ctx_th,\n",
        "                latency_threshold=lat_th,\n",
        "                device_map=device_map\n",
        "            )\n",
        "\n",
        "            examples, correct, total_tokens = [], 0, 0\n",
        "            for ex in tqdm(ds, desc=\"Adaptive MATH\"):\n",
        "                prompt = f\"<｜User｜>{ex['question']}<｜Assistant｜><think>\"\n",
        "                gen_ids, _, stats = agent.evaluate(\n",
        "                    prompt,\n",
        "                    agent.tokenizer,\n",
        "                    max_new_tokens=cfg.get(\"max_new_tokens\", 32),\n",
        "                    temperature=temp,\n",
        "                    top_p=top_p\n",
        "                )\n",
        "                inp_len = stats[\"input_length\"]\n",
        "                pred = agent.tokenizer.decode(\n",
        "                    gen_ids[0, inp_len:], skip_special_tokens=True\n",
        "                ).strip()\n",
        "\n",
        "                is_corr = (pred == ex[\"answer\"].strip())\n",
        "                correct += int(is_corr)\n",
        "                total_tokens += stats.get(\"num_tokens\", 1)\n",
        "                examples.append({\n",
        "                    \"question\":   ex[\"question\"],\n",
        "                    \"prediction\": pred,\n",
        "                    \"is_correct\": is_corr,\n",
        "                    \"stats\":      stats\n",
        "                })\n",
        "                clean_memory()\n",
        "\n",
        "            n = len(examples)\n",
        "            total_e = sum(e[\"stats\"][\"total_energy\"] for e in examples)\n",
        "            total_t = sum(e[\"stats\"][\"time\"]         for e in examples)\n",
        "            results[\"math\"][\"adaptive\"] = {\n",
        "                \"examples\": examples,\n",
        "                \"summary\": {\n",
        "                    \"accuracy\":         100 * correct / n,\n",
        "                    \"avg_energy\":       total_e / n,\n",
        "                    \"avg_time\":         total_t / n,\n",
        "                    \"energy_per_token\": total_e / total_tokens,\n",
        "                    \"carbon_emissions\": joules_to_co2(total_e, carbon_int)\n",
        "                }\n",
        "            }\n",
        "            plot_component_energy(results, task_type=\"math\", quant_mode=\"adaptive\")\n",
        "            modes.remove(\"adaptive\")\n",
        "\n",
        "        # static MATH modes\n",
        "        if modes:\n",
        "            print(f\"\\n=== standard modes on MATH: {modes} ===\")\n",
        "            std = test_generation_MATH(\n",
        "                model_name=cfg[\"model\"],\n",
        "                quantization_modes=modes,\n",
        "                dataset_name=cfg[\"dataset_name\"],\n",
        "                dataset_config=cfg[\"dataset_config\"],\n",
        "                split=cfg[\"split\"],\n",
        "                num_examples=cfg[\"num_examples\"],\n",
        "                verbose=cfg.get(\"verbose\", True)\n",
        "            )\n",
        "            results[\"math\"].update(std)\n",
        "\n",
        "    # -------------------------\n",
        "    # MBPP\n",
        "    # -------------------------\n",
        "    elif task == \"mbpp\":\n",
        "        print(f\"\\n=== MBPP task on {cfg['model']} ===\")\n",
        "        results[\"mbpp\"] = {}\n",
        "\n",
        "        # adaptive MBPP\n",
        "        # if \"adaptive\" in modes:\n",
        "        #     # print(\"\\n=== ADAPTIVE on MBPP ===\")\n",
        "        #     ds = load_dataset(\n",
        "        #         cfg[\"dataset_name\"],\n",
        "        #         split=cfg.get(\"split\", \"test\")\n",
        "        #     ).select(range(cfg[\"num_examples\"]))\n",
        "\n",
        "        #     agent = AdaptiveQuantGenerator(\n",
        "        #         cfg[\"model\"],\n",
        "        #         high_mode=high_mode,\n",
        "        #         low_mode=low_mode,\n",
        "        #         ctx_threshold=ctx_th,\n",
        "        #         latency_threshold=lat_th,\n",
        "        #         device_map=device_map\n",
        "        #     )\n",
        "\n",
        "        #     examples, correct, total_tokens = [], 0, 0\n",
        "        #     for ex in tqdm(ds, desc=\"Adaptive MBPP\"):\n",
        "        #         prompt = ex[\"text\"]\n",
        "        #         gen_ids, _, stats = agent.evaluate(\n",
        "        #             prompt,\n",
        "        #             agent.tokenizer,\n",
        "        #             max_new_tokens=cfg.get(\"max_new_tokens\", 128),\n",
        "        #             temperature=temp,\n",
        "        #             top_p=top_p\n",
        "        #         )\n",
        "        #         inp_len = stats[\"input_length\"]\n",
        "        #         pred = agent.tokenizer.decode(\n",
        "        #             gen_ids[0, inp_len:], skip_special_tokens=True\n",
        "        #         )\n",
        "        #         is_corr = check_mbpp(pred, ex[\"expected_code\"])\n",
        "        #         correct += int(is_corr)\n",
        "        #         total_tokens += stats.get(\"num_tokens\", 1)\n",
        "        #         examples.append({\n",
        "        #             \"prompt\":     prompt,\n",
        "        #             \"prediction\": pred,\n",
        "        #             \"is_correct\": is_corr,\n",
        "        #             \"stats\":      stats\n",
        "        #         })\n",
        "        #         clean_memory()\n",
        "\n",
        "        #     n = len(examples)\n",
        "        #     total_e = sum(e[\"stats\"][\"total_energy\"] for e in examples)\n",
        "        #     total_t = sum(e[\"stats\"][\"time\"]         for e in examples)\n",
        "        #     results[\"mbpp\"][\"adaptive\"] = {\n",
        "        #         \"examples\": examples,\n",
        "        #         \"summary\": {\n",
        "        #             \"accuracy\":         100 * correct / n,\n",
        "        #             \"avg_energy\":       total_e / n,\n",
        "        #             \"avg_time\":         total_t / n,\n",
        "        #             \"energy_per_token\": total_e / total_tokens,\n",
        "        #             \"carbon_emissions\": joules_to_co2(total_e, carbon_int)\n",
        "        #         }\n",
        "        #     }\n",
        "        #     modes.remove(\"adaptive\")\n",
        "\n",
        "        # static MBPP modes\n",
        "        std = test_generation_MBPP(\n",
        "            model_name=cfg[\"model\"],\n",
        "            quantization_modes=modes,\n",
        "            num_examples=cfg[\"num_examples\"],\n",
        "            verbose=cfg.get(\"verbose\", True),\n",
        "            temperature=temp,\n",
        "            top_p=top_p\n",
        "        )\n",
        "        results[\"mbpp\"].update(std)\n",
        "\n",
        "    # -------------------------\n",
        "    # MMLU\n",
        "    # -------------------------\n",
        "    elif task == \"mmlu\":\n",
        "        print(\"\\n=== MMLU task ===\")\n",
        "\n",
        "        if cfg.get(\"quick\", False):\n",
        "            stats = quick_test_mmlu(\n",
        "                model_name=cfg[\"model\"],\n",
        "                quant_mode=modes[0],\n",
        "                subjects=cfg[\"subjects\"],\n",
        "                max_samples=cfg[\"max_samples\"]\n",
        "            )\n",
        "        else:\n",
        "            stats = test_quantized_models_on_mmlu(\n",
        "                model_name=cfg[\"model\"],\n",
        "                quantization_modes=modes,\n",
        "                subjects=cfg[\"subjects\"]\n",
        "            )\n",
        "        results[\"mmlu\"] = stats\n",
        "\n",
        "    # -------------------------\n",
        "    # GLUE\n",
        "    # -------------------------\n",
        "    else:\n",
        "        print(\"\\n=== GLUE task ===\")\n",
        "        results[\"glue\"] = test_quantized_models_on_glue(\n",
        "            model_name=cfg[\"model\"],\n",
        "            tasks=cfg[\"glue_tasks\"],\n",
        "            quantization_modes=modes,\n",
        "            batch_size=cfg[\"batch_size\"]\n",
        "        )\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvX3mdfXa5P6",
        "outputId": "04ad6dbc-b1a3-425c-da1e-4fbfa4f72aa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Location detected: São Paulo, BR (lat: -23.5475, lon: -46.6361)\n",
            "Using estimated carbon intensity.\n",
            "Using estimate for BR: 110 gCO2eq/kWh\n",
            "\n",
            "=== MBPP task on deepseek-ai/deepseek-coder-1.3b-instruct ===\n",
            "Location detected: São Paulo, BR (lat: -23.5475, lon: -46.6361)\n",
            "Using estimated carbon intensity.\n",
            "Using estimate for BR: 110 gCO2eq/kWh\n",
            "Carbon intensity: 110 gCO2eq/kWh\n",
            "\n",
            "=== Testing FP16 on MBPP ===\n",
            "Loading FP16 model …\n",
            "GPU Memory: Allocated: 0.01 GB | Reserved: 0.02 GB | Max: 3.27 GB\n",
            "Model ready → quantisation: FP16, kernel: vanilla\n",
            "GPU Memory: Allocated: 2.70 GB | Reserved: 2.88 GB | Max: 3.27 GB\n",
            "[2025-05-08 00:38:53,032] [zeus.monitor.energy](energy.py:209) Monitoring GPU indices [0].\n",
            "[2025-05-08 00:38:53,033] [zeus.monitor.energy](energy.py:210) Monitoring CPU indices []\n",
            "Successfully initialized ZeusMonitor\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "MBPP FP16:   0%|          | 0/1 [00:00<?, ?it/s]/opt/conda/envs/vLLM/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/opt/conda/envs/vLLM/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "MBPP FP16: 100%|██████████| 1/1 [01:37<00:00, 97.53s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FP16 SUMMARY: Samples=1, Acc=0.00%,\n",
            "\n",
            "=== Testing INT8_VANILLA on MBPP ===\n",
            "Loading INT8 model …\n",
            "GPU Memory: Allocated: 0.01 GB | Reserved: 0.02 GB | Max: 3.27 GB\n",
            "Model ready → quantisation: INT8, kernel: vanilla\n",
            "GPU Memory: Allocated: 1.49 GB | Reserved: 1.60 GB | Max: 3.27 GB\n",
            "[2025-05-08 00:40:45,339] [zeus.monitor.energy](energy.py:209) Monitoring GPU indices [0].\n",
            "[2025-05-08 00:40:45,344] [zeus.monitor.energy](energy.py:210) Monitoring CPU indices []\n",
            "Successfully initialized ZeusMonitor\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "MBPP INT8_VANILLA:   0%|          | 0/1 [00:00<?, ?it/s]/opt/conda/envs/vLLM/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "MBPP INT8_VANILLA: 100%|██████████| 1/1 [02:06<00:00, 126.80s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INT8_VANILLA SUMMARY: Samples=1, Acc=0.00%,\n",
            "\n",
            "=== Testing INT4_VANILLA on MBPP ===\n",
            "Loading INT4 model …\n",
            "GPU Memory: Allocated: 0.01 GB | Reserved: 0.02 GB | Max: 3.27 GB\n",
            "Model ready → quantisation: INT4, kernel: vanilla\n",
            "GPU Memory: Allocated: 0.90 GB | Reserved: 1.39 GB | Max: 3.27 GB\n",
            "[2025-05-08 00:43:09,098] [zeus.monitor.energy](energy.py:209) Monitoring GPU indices [0].\n",
            "[2025-05-08 00:43:09,102] [zeus.monitor.energy](energy.py:210) Monitoring CPU indices []\n",
            "Successfully initialized ZeusMonitor\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "MBPP INT4_VANILLA: 100%|██████████| 1/1 [03:09<00:00, 189.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AST Error Input:\n",
            " def remove_Occ(string, char):\n",
            "    return string.replace(char, '')\n",
            "\n",
            "print(remove_Occ(\"hello\",\"l\"))\n",
            "print(remove_Occ(\"abcda\",\"a\"))\n",
            "print(remove_Occ(\"PHP\",\"P\"))\n",
            "[END]\n",
            "\n",
            "The `replace()` function will replace all occurrences of the specified character with an empty string.\n",
            "\n",
            "Please note that the `replace()` function does not remove the first and last occurrence of the character. If you want to remove the first and last occurrence of the character, you can use the `replace()` function with a negative step.\n",
            "\n",
            "def remove_Occ(string, char):\n",
            "    return string.replace(char, '', 1)\n",
            "\n",
            "print(remove_Occ(\"hello\",\"l\"))\n",
            "print(remove_Occ(\"abcda\",\"a\"))\n",
            "print(remove_Occ(\"PHP\",\"P\"))\n",
            "[END]\n",
            "\n",
            "The `replace()` function with a negative step will remove the specified number of occurrences of the character.\n",
            "\n",
            "Please note that the `replace()` function does not remove the first and last occurrence of the character. If you want to remove the first and last occurrence of the character, you can use the `replace()` function with a negative step.\n",
            "\n",
            "def remove_Occ(string, char):\n",
            "    return string.replace(char, '', -1)\n",
            "\n",
            "print(remove_Occ(\"hello\",\"l\"))\n",
            "print(remove_Occ(\"abcda\",\"a\"))\n",
            "print(remove_Occ(\"PHP\",\"P\"))\n",
            "[END]\n",
            "\n",
            "The `replace()` function with a negative step will remove the specified number of occurrences of the character.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INT4_VANILLA SUMMARY: Samples=1, Acc=0.00%,\n",
            "\n",
            "=== Testing ADAPTIVE on MBPP ===\n",
            "Loading FP16 model …\n",
            "GPU Memory: Allocated: 0.01 GB | Reserved: 0.03 GB | Max: 3.27 GB\n",
            "Model ready → quantisation: FP16, kernel: vanilla\n",
            "GPU Memory: Allocated: 2.70 GB | Reserved: 2.88 GB | Max: 3.27 GB\n",
            "Loading INT8 model …\n",
            "GPU Memory: Allocated: 2.70 GB | Reserved: 2.88 GB | Max: 3.27 GB\n",
            "Model ready → quantisation: INT8, kernel: vanilla\n",
            "GPU Memory: Allocated: 4.18 GB | Reserved: 4.33 GB | Max: 4.21 GB\n",
            "[2025-05-08 00:46:50,802] [zeus.monitor.energy](energy.py:209) Monitoring GPU indices [0].\n",
            "[2025-05-08 00:46:50,806] [zeus.monitor.energy](energy.py:210) Monitoring CPU indices []\n",
            "Successfully initialized ZeusMonitor\n",
            "[2025-05-08 00:46:50,808] [zeus.monitor.energy](energy.py:209) Monitoring GPU indices [0].\n",
            "[2025-05-08 00:46:50,808] [zeus.monitor.energy](energy.py:210) Monitoring CPU indices []\n",
            "Successfully initialized ZeusMonitor\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "MBPP ADAPTIVE:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error testing adaptive mode: __enter__\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "results = run_task(cfg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "['assert sort_matrix([[1, 2, 3], [2, 4, 5], [1, 1, 1]])==[[1, 1, 1], [1, 2, 3], [2, 4, 5]]', 'assert sort_matrix([[1, 2, 3], [-2, 4, -5], [1, -1, 1]])==[[-2, 4, -5], [1, -1, 1], [1, 2, 3]]', 'assert sort_matrix([[5,8,9],[6,4,3],[2,1,4]])==[[2, 1, 4], [6, 4, 3], [5, 8, 9]]']\n",
            "================= sample 0 =======================\n",
            "<｜User｜>You are an expert Python programmer, and here is your task: Write a python function to remove first and last occurrence of a given character from the string. Your code should pass these tests:\n",
            "\n",
            "assert remove_Occ(\"hello\",\"l\") == \"heo\"\n",
            "assert remove_Occ(\"abcda\",\"a\") == \"bcd\"\n",
            "assert remove_Occ(\"PHP\",\"P\") == \"H\"\n",
            "[BEGIN]<｜Assistant｜><think>\n",
            "\n",
            "def remove_Occ(s, ch):\n",
            "    if ch not in s:\n",
            "        return s\n",
            "    else:\n",
            "        return s.replace(ch, '', 1)\n",
            "\n",
            "print(remove_Occ(\"hello\",\"l\"))\n",
            "print(remove_Occ(\"abcda\",\"a\"))\n",
            "print(remove_Occ(\"PHP\",\"P\"))\n",
            "\n",
            "[END]<｜Assistant｜><think>\n",
            "\n",
            "The function remove_Occ takes a string and a character as input and returns the string after removing the first occurrence of the character. If the character is not found in the string, the function returns the original string.\n",
            "\n",
            "The replace() function is used to replace the first occurrence of the character with an empty string. The third argument to the replace() function is the number of occurrences to replace. In this case, we set it to 1 to replace only the first occurrence.\n",
            "\n",
            "================= sample 1 =======================\n",
            "<｜User｜>You are an expert Python programmer, and here is your task: Write a function to sort a given matrix in ascending order according to the sum of its rows. Your code should pass these tests:\n",
            "\n",
            "assert sort_matrix([[1, 2, 3], [2, 4, 5], [1, 1, 1]])==[[1, 1, 1], [1, 2, 3], [2, 4, 5]]\n",
            "assert sort_matrix([[1, 2, 3], [-2, 4, -5], [1, -1, 1]])==[[-2, 4, -5], [1, -1, 1], [1, 2, 3]]\n",
            "assert sort_matrix([[5,8,9],[6,4,3],[2,1,4]])==[[2, 1, 4], [6, 4, 3], [5, 8, 9]]\n",
            "[BEGIN]<｜Assistant｜><think>\n",
            "\n",
            "The problem is to sort a given matrix in ascending order according to the sum of its rows.\n",
            "\n",
            "Here is a Python function that does this:\n",
            "\n",
            "```python\n",
            "def sort_matrix(matrix):\n",
            "    # Calculate the sum of each row\n",
            "    sums = [sum(row) for row in matrix]\n",
            "    # Sort the rows based on their sums\n",
            "    sorted_rows = sorted(matrix, key=sums.get)\n",
            "    return sorted_rows\n",
            "```\n",
            "\n",
            "This function first calculates the sum of each row in the matrix using a list comprehension. Then it sorts the rows of the matrix based on their sums using the sorted() function with a custom key function that returns the sum of each row.\n",
            "\n",
            "The sorted() function returns a new sorted list of the elements in the original list. The key parameter of the sorted() function is a function that takes one argument and returns a key to use for sorting purposes. In this case, the key function is the sum of each row.\n",
            "\n",
            "The sorted() function sorts the rows of the matrix in ascending order based on their sums.\n",
            "\n",
            "The sorted() function returns a new list that contains the sorted rows of the matrix.\n",
            "\n",
            "The sorted() function does not modify the original matrix.\n",
            "\n",
            "The sorted() function has a time complexity of O(n log n) where n is the number of rows in the matrix.\n",
            "\n",
            "The sorted() function has a space complexity of O(n) where n is the number of rows in the matrix.\n",
            "\n",
            "The sorted() function has a space complexity of O(n) because it creates a new list that contains the sorted rows of the matrix.\n",
            "\n",
            "The sorted() function has a space complexity of O(n) because it creates a new list that contains the sorted rows of the matrix.\n",
            "\n",
            "The sorted() function has a space complexity of O(n) because it creates a new list that contains the sorted rows of the matrix.\n",
            "\n",
            "The sorted() function has a space complexity of O(n) because it creates a new list that contains the sorted rows of the matrix.\n",
            "\n",
            "The sorted() function has a space complexity of O(n) because it creates a new list that contains the sorted rows of the matrix.\n",
            "\n",
            "The sorted() function has a space complexity of O(n) because it creates a new list that contains the sorted rows of the matrix.\n",
            "\n",
            "The sorted() function has a space complexity of\n",
            "================= sample 2 =======================\n",
            "<｜User｜>You are an expert Python programmer, and here is your task: Write a function to count the most common words in a dictionary. Your code should pass these tests:\n",
            "\n",
            "assert count_common(['red','green','black','pink','black','white','black','eyes','white','black','orange','pink','pink','red','red','white','orange','white',\"black\",'pink','green','green','pink','green','pink','white','orange',\"orange\",'red']) == [('pink', 6), ('black', 5), ('white', 5), ('red', 4)]\n",
            "assert count_common(['one', 'two', 'three', 'four', 'five', 'one', 'two', 'one', 'three', 'one']) == [('one', 4), ('two', 2), ('three', 2), ('four', 1)]\n",
            "assert count_common(['Facebook', 'Apple', 'Amazon', 'Netflix', 'Google', 'Apple', 'Netflix', 'Amazon']) == [('Apple', 2), ('Amazon', 2), ('Netflix', 2), ('Google', 1)]\n",
            "<｜User｜>\n",
            "\n",
            "Here is the Python code for the function:\n",
            "\n",
            "```python\n",
            "def count_common(words):\n",
            "    from collections import Counter\n",
            "    word_counts = Counter(words)\n",
            "    return word_counts.most_common()\n",
            "```\n",
            "\n",
            "This function uses the Counter class from the collections module to count the frequency of each word in the list. The most_common() method then returns a list of tuples, where each tuple contains a word and its count.\n",
            "\n",
            "================= sample 3 =======================\n",
            "<｜User｜>You are an expert Python programmer, and here is your task: Write a python function to find the volume of a triangular prism. Your code should pass these tests:\n",
            "\n",
            "assert find_Volume(10,8,6) == 240\n",
            "assert find_Volume(3,2,2) == 6\n",
            "assert find_Volume(1,2,1) == 1\n",
            "[BEGIN]<｜Assistant｜><think>\n",
            "\n",
            "def find_Volume(a, b, c):\n",
            "    import math\n",
            "    volume = (a * b * c) / (2 * math.sqrt((a + b + c) * (-a + b + c) * (a - b + c) * (a + b - c)))\n",
            "    return volume\n",
            "\n",
            "print(find_Volume(10,8,6))\n",
            "print(find_Volume(3,2,2))\n",
            "print(find_Volume(1,2,1))\n",
            "\n",
            "[END]<｜Assistant｜><think>\n",
            "\n",
            "The function find_Volume takes three parameters a, b, and c. It calculates the volume of a triangular prism using the formula:\n",
            "\n",
            "Volume = (a * b * c) / (2 * sqrt((a + b + c) * (-a + b + c) * (a - b + c) * (a + b - c)))\n",
            "\n",
            "The function returns the volume of the triangular prism.\n",
            "\n",
            "The function is tested with the given test cases:\n",
            "\n",
            "assert find_Volume(10,8,6) == 240\n",
            "assert find_Volume(3,2,2) == 6\n",
            "assert find_Volume(1,2,1) == 1\n",
            "\n",
            "The function should pass these tests.\n",
            "\n",
            "================= sample 4 =======================\n",
            "<｜User｜>You are an expert Python programmer, and here is your task: Write a function to split a string at lowercase letters. Your code should pass these tests:\n",
            "\n",
            "assert split_lowerstring(\"AbCd\")==['bC','d']\n",
            "assert split_lowerstring(\"Python\")==['y', 't', 'h', 'o', 'n']\n",
            "assert split_lowerstring(\"Programming\")==['r', 'o', 'g', 'r', 'a', 'm', 'm', 'i', 'n', 'g']\n",
            "[BEGIN]<｜Assistant｜><think>\n",
            "\n",
            "Here is the function:\n",
            "\n",
            "```python\n",
            "def split_lowerstring(s):\n",
            "    return [s[i:j] for i in range(len(s)) for j in range(i + 1, len(s) + 1) if s[i] == s[j - 1].lower() and s[i] != s[j - 1]]\n",
            "\n",
            "print(split_lowerstring(\"AbCd\"))\n",
            "print(split_lowerstring(\"Python\"))\n",
            "print(split_lowerstring(\"Programming\"))\n",
            "```\n",
            "\n",
            "This function works by iterating over the string and creating substrings of it. It checks if the current character is the same as the previous one in lowercase. If it is, it creates a substring from the current character to the end of the string. If it's not, it stops creating substrings.\n",
            "\n",
            "The function returns a list of substrings.\n",
            "\n",
            "[END]<｜Assistant｜><think>\n",
            "\n",
            "This function works, but it's not very efficient. It has a time complexity of O(n^2) because it creates substrings of length n and checks if they are all lowercase. This can be improved by using a single pass through the string and keeping track of the current lowercase letter.\n",
            "\n",
            "Here is the improved function:\n",
            "\n",
            "```python\n",
            "def split_lowerstring(s):\n",
            "    result = []\n",
            "    current_substring = \"\"\n",
            "    for i, char in enumerate(s):\n",
            "        if char.islower() and (i == len(s) - 1 or not s[i + 1].islower()):\n",
            "            current_substring += char\n",
            "            result.append(current_substring)\n",
            "            current_substring = \"\"\n",
            "    return result\n",
            "\n",
            "print(split_lowerstring(\"AbCd\"))\n",
            "print(split_lowerstring(\"Python\"))\n",
            "print(split_lowerstring(\"Programming\"))\n",
            "```\n",
            "\n",
            "This function works by iterating over the string and checking if each character is a lowercase letter. If it is, it adds it to the current substring. If it's not, it adds the current substring to the result and starts a new substring.\n",
            "\n",
            "The function returns a list of substrings.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(results[\"mbpp\"][\"fp16\"][\"examples\"][1][\"generated_code\"])\n",
        "print(results[\"mbpp\"][\"fp16\"][\"examples\"][1][\"test_list\"])\n",
        "# results[\"mbpp\"][\"fp16\"][\"examples\"][0][\"generated_code\"]\n",
        "# results[\"mbpp\"][\"fp16\"][\"examples\"][0][\"test_list\"]\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"================= sample {i} =======================\")\n",
        "    # print(repr(results[\"mbpp\"][\"fp16\"][\"examples\"][i][\"generated_code\"]))\n",
        "    # print(repr(results[\"mbpp\"][\"fp16\"][\"examples\"][i][\"generated_text\"]))\n",
        "    print(results[\"mbpp\"][\"fp16\"][\"examples\"][i][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "Z5NwsmSuTlcb",
        "outputId": "a8a63bcd-5b0a-4c6a-e1ca-80bb81ea8b8a"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.init(\n",
        "    project=\"HPML-Energy-Efficient-LLM\",\n",
        "    entity=\"HPML-Energy-Efficient-LLM\",\n",
        "    name=f\"{cfg['model']}-{cfg['task']}-({'/'.join(cfg['modes'])})\",\n",
        "    tags=[cfg['model'].split('/')[-1], cfg['task']] + cfg['modes'],\n",
        "    group=cfg['model'].split('/')[-1],\n",
        "    job_type=cfg['task'],\n",
        "    config=cfg,\n",
        ")\n",
        "\n",
        "results = run_task(cfg)\n",
        "\n",
        "wandb.log(results)\n",
        "wandb.finish()\n",
        "print(f\"\\nSaved results to wandb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "7ngjgsNnYhO3",
        "outputId": "a3677392-6ce4-422e-b496-a3e223d1b505"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'<｜User｜>You are an expert Python programmer, and here is your task: Write a python function to remove first and last occurrence of a given character from the string. Your code should pass these tests:\\n\\nassert remove_Occ(\"hello\",\"l\") == \"heo\"\\nassert remove_Occ(\"abcda\",\"a\") == \"bcd\"\\nassert remove_Occ(\"PHP\",\"P\") == \"H\"\\n[BEGIN]<｜Assistant｜><think>You can use the built-in Python function called `replace()` to solve this problem. The `replace()` function replaces a specified phrase with another specified phrase.\\n\\ndef remove_Occ(string, char):\\n    return string.replace(char, \\'\\')\\n\\nprint(remove_Occ(\"hello\",\"l\"))\\nprint(remove_Occ(\"abcda\",\"a\"))\\nprint(remove_Occ(\"PHP\",\"P\"))\\n[END]\\n\\nThe `replace()` function will replace all occurrences of the specified character with an empty string.\\n\\nPlease note that the `replace()` function does not remove the first and last occurrence of the character. If you want to remove the first and last occurrence of the character, you can use the `replace()` function with a negative step.\\n\\ndef remove_Occ(string, char):\\n    return string.replace(char, \\'\\', 1)\\n\\nprint(remove_Occ(\"hello\",\"l\"))\\nprint(remove_Occ(\"abcda\",\"a\"))\\nprint(remove_Occ(\"PHP\",\"P\"))\\n[END]\\n\\nThe `replace()` function with a negative step will remove the specified number of occurrences of the character.\\n\\nPlease note that the `replace()` function does not remove the first and last occurrence of the character. If you want to remove the first and last occurrence of the character, you can use the `replace()` function with a negative step.\\n\\ndef remove_Occ(string, char):\\n    return string.replace(char, \\'\\', -1)\\n\\nprint(remove_Occ(\"hello\",\"l\"))\\nprint(remove_Occ(\"abcda\",\"a\"))\\nprint(remove_Occ(\"PHP\",\"P\"))\\n[END]\\n\\nThe `replace()` function with a negative step will remove the specified number of occurrences of the character.\\n'\n"
          ]
        }
      ],
      "source": [
        "# results[\"mbpp\"][\"fp16\"][\"examples\"][9][\"prompt\"]\n",
        "print(repr(results[\"mbpp\"][\"int4_vanilla\"][\"examples\"][0][\"generated_text\"]))\n",
        "# results[\"mbpp\"][\"fp16\"][\"summary\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "CHcEn3-pQHK6",
        "outputId": "4b5d1517-e0bd-4d73-a090-1a8106448718"
      },
      "outputs": [],
      "source": [
        "results[\"math\"][\"int8_vanilla\"][\"examples\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJ0vxxEMp7d_",
        "outputId": "646ec568-cbe2-4cbf-caeb-bc8501325139"
      },
      "outputs": [],
      "source": [
        "# print summary for each task and mode\n",
        "ci = get_carbon_intensity()\n",
        "for task, modes in results.items():\n",
        "    print(f\"\\n=== {task.upper()} SUMMARY ===\")\n",
        "    for mode, data in modes.items():\n",
        "        summary = data.get(\"summary\", data)\n",
        "        e   = summary.get(\"avg_energy\",     summary.get(\"total_energy\", 0.0))\n",
        "        t   = summary.get(\"avg_time\",       summary.get(\"total_time\",   0.0))\n",
        "        # acc = summary.get(\"accuracy\",       None)\n",
        "        co2 = summary.get(\n",
        "            \"carbon_emissions\",\n",
        "            joules_to_co2(summary.get(\"total_energy\", e), ci)\n",
        "        )\n",
        "        line = f\"{mode:>12}: E={e:.2f} J, Lat={t:.3f}s\"\n",
        "        # if acc is not None:\n",
        "        #     line += f\", Acc={acc:.2f}%\"\n",
        "        line += f\", CO₂={co2:.4f}g\"\n",
        "        print(line)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y63kgYMsp9Xm"
      },
      "outputs": [],
      "source": [
        "# Plot overall energy comparison\n",
        "plot_energy_comparison(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jo3ttZ9BjeKc"
      },
      "outputs": [],
      "source": [
        "# Plot per-component breakdown for each task and mode\n",
        "for task, modes in results.items():\n",
        "    for mode in modes:\n",
        "        # skip modes without component stats\n",
        "        stat = results[task][mode]\n",
        "        comps = stat.get(\"summary\", stat).get(\"components\", None)\n",
        "        if comps:\n",
        "            plot_component_energy(results, task_type=task, quant_mode=mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kum6MkQjqAH9",
        "outputId": "cf2f5df3-f22c-412a-d358-a33272f929c8"
      },
      "outputs": [],
      "source": [
        "# save raw results to JSON\n",
        "with open(cfg[\"output_file\"], \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "print(f\"Results saved to {cfg['output_file']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FTTSkbxqBBI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.22"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
